{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import variation\n",
    "import glob\n",
    "import os\n",
    "import typingmod as typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## mounting to ION server\n",
    "# os.system(\"osascript -e 'mount volume \\\"smb://ion-nas.uoregon.edu\\\" \\\n",
    "#           as user name \\\"greenhouse\\\" with password \\\"password\\\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining function to organize bigrams into rows\n",
    "def bigram_byrow():\n",
    "    bigrams = []\n",
    "    for index, row in keys_intocolumns.iterrows():\n",
    "        for column in range(0, (len(keys_intocolumns.columns) - 1)):\n",
    "            if (keys_intocolumns[column][index] != None and float('nan')) and (keys_intocolumns[column + 1][index] != None and float('nan')):\n",
    "                bigram = (keys_intocolumns[column][index] + keys_intocolumns[column + 1][index])\n",
    "                bigram = (bigram.replace(\"'\", \"\")).replace(\" \", \"\")\n",
    "                iki = (main_df['key_resp.rt.%(second)d' % {'second':  column + 2 }][index] - main_df['key_resp.rt.%(first)d' % { 'first': column +1 }][index])\n",
    "                bigrams.append([index, column, bigram, iki, main_df['string'][index], main_df['resp_string'][index]])\n",
    "    return(bigrams)\n",
    "\n",
    "## defining function that separates words in to bigrams\n",
    "def bi_byword(word):\n",
    "    bi_results = []\n",
    "    for y in range(0, (len(word)-1)):\n",
    "        bigram = word[y] + word[y+1]\n",
    "        bi_results.append(bigram)\n",
    "    return bi_results\n",
    "\n",
    "## defining function that separates all words into bigrams\n",
    "def bi_allwords():\n",
    "    bigrams = []\n",
    "    for word in df['string']:\n",
    "        bigrams.append(bi_byword(word))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/greenhouse/typingtask_data/subject_data/s262_01232024/psychopy_data/edited/s262_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s262_01232024/psychopy_data/edited/s262_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s261_12122023/psychopy_data/edited/s261_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s261_12122023/psychopy_data/edited/s261_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s240_11162023/psychopy_data/edited/s240_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s240_11162023/psychopy_data/edited/s240_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s217_11092023/psychopy_data/edited/s217_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s217_11092023/psychopy_data/edited/s217_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s176_10262023/psychopy_data/edited/s176_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s176_10262023/psychopy_data/edited/s176_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s20_02082024/psychopy_data/edited/s20_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s20_02082024/psychopy_data/edited/s20_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s267_02122024/psychopy_data/edited/s267_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s267_02122024/psychopy_data/edited/s267_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s263_01312024/psychopy_data/edited/s263_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s263_01312024/psychopy_data/edited/s263_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s254_11022023/psychopy_data/edited/s254_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s254_11022023/psychopy_data/edited/s254_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s09_02162024/psychopy_data/edited/s09_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s09_02162024/psychopy_data/edited/s09_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s209_09202023/psychopy_data/edited/s209_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s209_09202023/psychopy_data/edited/s209_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s276_02212024/psychopy_data/edited/s276_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s276_02212024/psychopy_data/edited/s276_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s278_02232024/psychopy_data/edited/s278_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s278_02232024/psychopy_data/edited/s278_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s279_02232024/psychopy_data/edited/s279_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s279_02232024/psychopy_data/edited/s279_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s282_02292024/psychopy_data/edited/s282_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s282_02292024/psychopy_data/edited/s282_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s283_02292024/psychopy_data/edited/s283_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s283_02292024/psychopy_data/edited/s283_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s286_03082024/psychopy_data/edited/s286_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s286_03082024/psychopy_data/edited/s286_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s302_05282024/psychopy_data/edited/s302_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s302_05282024/psychopy_data/edited/s302_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s304_07102024/psychopy_data/edited/s304_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s304_07102024/psychopy_data/edited/s304_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s306_07122024/psychopy_data/edited/s306_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s306_07122024/psychopy_data/edited/s306_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s305_07112024/psychopy_data/edited/s305_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s305_07112024/psychopy_data/edited/s305_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s309_09232024/psychopy_data/edited/s309_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s309_09232024/psychopy_data/edited/s309_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s311_10152024/psychopy_data/edited/s311_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s311_10152024/psychopy_data/edited/s311_bybigram.csv\n"
     ]
    }
   ],
   "source": [
    "## create dataframes tiral-based and bigram-based dataframes for each subject ##\n",
    "\n",
    "## importing experiment data\n",
    "server = r'/Volumes/greenhouse/typingtask_data/subject_data'\n",
    "server_noturbo = r'/Volumes/greenhouse/typingtask_data/subject_data/not_used/no_turbotyping/'\n",
    "os.chdir(server)\n",
    "folders = os.listdir()\n",
    "\n",
    "# looping through subjects\n",
    "sub_folders = list(filter(lambda x: x.startswith('s', 0, 1), folders))\n",
    "for sub in sub_folders:\n",
    "    sub_folder = r'/Volumes/greenhouse/typingtask_data/subject_data/%s/psychopy_data/' % sub\n",
    "    os.chdir(sub_folder)\n",
    "    sID = sub.split('_', 1)[0]\n",
    "    og_df = pd.read_csv(glob.glob('*.csv')[0])   \n",
    "\n",
    "## filters through subjects without turbotyping data\n",
    "# sub_folders = list(filter(lambda x: x.startswith('s', 0, 1), folders))\n",
    "# for sub in sub_folders:\n",
    "#     sub_folder = server_noturbo + r'%s/psychopy_data/' % sub\n",
    "#     os.chdir(sub_folder)\n",
    "#     sID = sub.split('_', 1)[0]\n",
    "#     og_df = pd.read_csv(glob.glob('*.csv')[0])  \n",
    "   \n",
    "    ## deleting first 3 practice trials -- EDIT FOR ANY TRIALS YOU WANT TO IMMEDIATELY EXCLUDE\n",
    "    df = (og_df.drop(labels=[0, 1, 2], axis=0)).reset_index(drop = True) \n",
    "    \n",
    "    ## expanding nested key_resp.rt values into separate columns, making new dataframe, and turning values back into floats from strings\n",
    "    stripped_rts_1 = ((df['key_resp_1.rt'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    stripped_rts_2 = ((df['key_resp_2.rt'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    rts_intocolumns = (pd.concat([stripped_rts_1, stripped_rts_2])).reset_index(drop = True)\n",
    "    \n",
    "    ## renames rt columns to automatically match dataset\n",
    "    DF = rts_intocolumns\n",
    "    renamed_rt = DF.rename(columns = { 0:'key_resp.rt.%s' %(0+1) })\n",
    "    for n in range(0, len(DF.columns)):\n",
    "        renamed_rt = renamed_rt.rename(columns = { n:'key_resp.rt.%s' %(n+1) })\n",
    "    expanded_rts = renamed_rt.astype(float).fillna(0) ##replacing NaNs with zeroes\n",
    "\n",
    "    ## expanding nested key_resp.keys values into separate columns and making new dataframe\n",
    "    stripped_keys_1 = ((df['key_resp_1.keys'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    stripped_keys_2 = ((df['key_resp_2.keys'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    keys_intocolumns = (pd.concat([stripped_keys_1, stripped_keys_2])).reset_index(drop = True)\n",
    "    keys_intocolumns = keys_intocolumns.where(pd.notnull(keys_intocolumns), None) \n",
    "        # ^ also replaces any added NaNs with Nones\n",
    "\n",
    "    ## renames key columns to automatically match dataset\n",
    "    DF = keys_intocolumns\n",
    "    expanded_keys = DF.rename(columns = { 0:'key_resp.keys.%s' %(0+1) })\n",
    "    for n in range(0, len(DF.columns)):\n",
    "        expanded_keys = expanded_keys.rename(columns = { n:'key_resp.keys.%s' %(n+1) })\n",
    "\n",
    "    ## getting rid of apostrophes and spaces in key values\n",
    "    cols_to_change = (expanded_keys.iloc[:, 0:])\n",
    "    for col in cols_to_change:\n",
    "        expanded_keys[col] = expanded_keys[col].str.replace(\"'\", \"\")\n",
    "        expanded_keys[col] = expanded_keys[col].str.replace(\" \", \"\")\n",
    "\n",
    "    ## combining key_resp.keys into one simple string to easily represent typed responses\n",
    "    responses_1 = pd.DataFrame((df['key_resp_1.keys'].str.replace(\"[', ]\", \"\", regex=True).str.strip(\"[]\")).dropna()).rename(columns = {'key_resp_1.keys':'resp_string'})\n",
    "    responses_2 = pd.DataFrame((df['key_resp_2.keys'].str.replace(\"[', ]\", \"\", regex=True).str.strip(\"[]\")).dropna()).rename(columns = {'key_resp_2.keys':'resp_string'})\n",
    "    responses = (pd.concat([responses_1, responses_2])).reset_index(drop = True)\n",
    "\n",
    "    ## identifying bigrams in words to add to larger dataframe\n",
    "    task_bigrams = pd.DataFrame(bi_allwords())\n",
    "    task_bigrams.columns = ['bi_1', 'bi_2', 'bi_3', 'bi_4']\n",
    "    \n",
    "    ## combining expanded rt, expanded keys, and response string values with column for strings typed each trial to create more useful dataframe\n",
    "    ## (does not have all the random timing data of other events occuring during the task)\n",
    "    main_df = pd.concat([responses, task_bigrams, expanded_keys, expanded_rts], axis = 1)\n",
    "    main_df.insert(0, 'string', df['string'], True)\n",
    "\n",
    "    ## creating column for WF type for each trial\n",
    "    main_df['wf_type'] = \"\"\n",
    "    for index, data in main_df.iterrows():\n",
    "        if main_df.loc[index, 'string'] in typ.highwf:\n",
    "            main_df.loc[index, 'wf_type'] = 'highwf'\n",
    "        if main_df.loc[index, 'string'] in typ.medwf:\n",
    "            main_df.loc[index, 'wf_type'] = 'medwf'\n",
    "        if main_df.loc[index, 'string'] in typ.lowwf:\n",
    "            main_df.loc[index, 'wf_type'] = 'lowwf'\n",
    "        if main_df.loc[index, 'string'] in typ.pseudo:\n",
    "            main_df.loc[index, 'wf_type'] = 'pseudo'\n",
    "\n",
    "    ## creating column for BF type for each trial\n",
    "    main_df['meanbf_type'] = \"\"\n",
    "    for index, data in main_df.iterrows():\n",
    "        if main_df.loc[index, 'string'] in typ.avg_highbf:\n",
    "            main_df.loc[index, 'meanbf_type'] = 'highbf'\n",
    "        if main_df.loc[index, 'string'] in typ.avg_medbf:\n",
    "            main_df.loc[index, 'meanbf_type'] = 'medbf'\n",
    "        if main_df.loc[index, 'string'] in typ.avg_lowbf:\n",
    "            main_df.loc[index, 'meanbf_type'] = 'lowbf'\n",
    "\n",
    "    ## creating column for trial (useful for group analysis)\n",
    "    trial_nums = []\n",
    "    for index, data in main_df.iterrows():\n",
    "        trial_nums.append(index)\n",
    "    main_df.insert(0, 'trial_num', trial_nums)\n",
    "\n",
    "    ## creating column for subject ID (also useful for group analysis)\n",
    "    main_ID = [sID]*len(main_df)\n",
    "    main_df.insert(0, 'sID', main_ID)\n",
    "\n",
    "    ## creating columns for word repetition number\n",
    "    main_df.insert(2, 'rep_num', '')\n",
    "    main_df['rep_num'] = main_df.groupby(['sID', 'string']).cumcount()\n",
    "\n",
    "    ## creating column for if trial is correct or not\n",
    "    main_df['trial_corr'] = ''\n",
    "    corr_trials = (main_df[main_df.string \n",
    "                   == main_df.resp_string])\n",
    "    corr_indices = list(corr_trials.index.values)\n",
    "    main_df.loc[corr_indices, 'trial_corr'] = \"corr\"\n",
    "    \n",
    "    incorr_trials = (main_df[main_df.string \n",
    "                     != main_df.resp_string])\n",
    "    incorr_indices = list(incorr_trials.index.values)\n",
    "    main_df.loc[incorr_indices, 'trial_corr'] = \"incorr\"\n",
    "    \n",
    "    ## making csv from dataframe\n",
    "    edited_path = os.path.join(sub_folder, 'edited')\n",
    "    if os.path.exists(edited_path) == False:\n",
    "        os.mkdir(edited_path)\n",
    "    bytrial_path = os.path.join(edited_path, '%s_bytrial.csv' % sID)\n",
    "    print(bytrial_path)\n",
    "    main_df.to_csv(bytrial_path)\n",
    "\n",
    "    \n",
    "    ## BIGRAM DATAFRAME ##\n",
    "    bigram_df = (pd.DataFrame(bigram_byrow())).rename(columns={0: \"trial_num\", 1: \"bigram_loc\",  2:\"resp_bigram\", 3: \"IKI\", 4: \"string\", 5: \"resp_string\"})\n",
    "\n",
    "    ## creating column for bigram # (useful for group analysis)\n",
    "    bigram_nums = []\n",
    "    for index, data in bigram_df.iterrows():\n",
    "        bigram_nums.append(index)\n",
    "    bigram_df.insert(0, 'bigram_num', bigram_nums)\n",
    "\n",
    "    ## creating column for subject ID (also useful for group analysis)\n",
    "    bigram_ID = [sID]*len(bigram_df)\n",
    "    bigram_df.insert(0, 'sID', bigram_ID)\n",
    "\n",
    "    ## creating column for correct bigram (as opposed to the typed bigram)\n",
    "    bigram_df.insert(4, 'bigram', '')\n",
    "    for index, row in bigram_df.iterrows():\n",
    "        loc = bigram_df.loc[index, 'bigram_loc']\n",
    "        loc_list = [0, 1, 2, 3]\n",
    "        if loc in loc_list:\n",
    "            corr = bi_byword(bigram_df.loc[index, 'string'])[loc]\n",
    "        else:\n",
    "            corr = ''\n",
    "        bigram_df.loc[index, 'bigram'] = corr\n",
    "\n",
    "    ## creating column for rep #\n",
    "    bigram_df.insert(3, 'rep_num', '')\n",
    "    bigram_df['rep_num'] = bigram_df.groupby(['sID', 'string', 'bigram']).cumcount()\n",
    "    \n",
    "    ## creating column for bigram frequency\n",
    "    bg_freqs = pd.read_csv(r'/Users/rubi/Desktop/Github/typingexp/typing_task_analysis/bg_freqs.csv') ## EDIT TO MAKE USEFUL ON OTHER COMPUTERS\n",
    "    bg_freqs.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "    freq_dict = bg_freqs.set_index('Bigrams')['Frequency'].to_dict()\n",
    "    bigram_df['bg_freq'] = bigram_df['bigram'].map(freq_dict)\n",
    "\n",
    "    ## creating column for bigram type\n",
    "    name_list = ['high', 'med', 'low', 'pseudo']\n",
    "\n",
    "    for index, bf_type in enumerate(typ.bf_types):\n",
    "        by_bf = bigram_df[bigram_df.bigram.isin(bf_type)]\n",
    "        rows = by_bf.index\n",
    "        bigram_df.loc[rows, 'bf_type'] = name_list[index]\n",
    "\n",
    "    ## creating a column for mean bigram type\n",
    "    for index, avgbf_type in enumerate(typ.avgbf_types):\n",
    "        by_bf = bigram_df[bigram_df.string.isin(avgbf_type)]\n",
    "        rows = by_bf.index\n",
    "        bigram_df.loc[rows, 'meanbf_type'] = name_list[index]\n",
    "\n",
    "    ## creating a column for mean bigram type\n",
    "    for index, wf_type in enumerate(typ.wf_types):\n",
    "        by_wf = bigram_df[bigram_df.string.isin(wf_type)]\n",
    "        rows = by_wf.index\n",
    "        bigram_df.loc[rows, 'wf_type'] = name_list[index]\n",
    "\n",
    "    ## creating column for if trial is correct or not\n",
    "    bigram_df['trial_corr'] = ''\n",
    "    corr_trials_bybg = (bigram_df[bigram_df.string \n",
    "                   == bigram_df.resp_string])\n",
    "    corr_indices_bybg = list(corr_trials_bybg.index.values)\n",
    "    bigram_df.loc[corr_indices_bybg, 'trial_corr'] = \"corr\"\n",
    "    \n",
    "    incorr_trials_bybg = (bigram_df[bigram_df.string \n",
    "                     != bigram_df.resp_string])\n",
    "    incorr_indices_bybg = list(incorr_trials_bybg.index.values)\n",
    "    bigram_df.loc[incorr_indices_bybg, 'trial_corr'] = \"incorr\"\n",
    "\n",
    "    ## creating column for if bigram is correct or not\n",
    "    bigram_df['bg_corr'] = ''\n",
    "    corr_bgs = (bigram_df[bigram_df.bigram \n",
    "                   == bigram_df.resp_bigram])\n",
    "    corr_bg_indices = list(corr_bgs.index.values)\n",
    "    bigram_df.loc[corr_bg_indices, 'bg_corr'] = \"corr\"\n",
    "    \n",
    "    incorr_bgs = (bigram_df[bigram_df.bigram \n",
    "                     != bigram_df.resp_bigram])\n",
    "    incorr_bg_indices = list(incorr_bgs.index.values)\n",
    "    bigram_df.loc[incorr_bg_indices, 'bg_corr'] = \"incorr\"\n",
    "\n",
    "    ## making csv from dataframe\n",
    "    bybigram_path = os.path.join(edited_path, '%s_bybigram.csv' % sID)\n",
    "    print(bybigram_path)\n",
    "    bigram_df.to_csv(bybigram_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sID</th>\n",
       "      <th>trial_num</th>\n",
       "      <th>rep_num</th>\n",
       "      <th>string</th>\n",
       "      <th>resp_string</th>\n",
       "      <th>bi_1</th>\n",
       "      <th>bi_2</th>\n",
       "      <th>bi_3</th>\n",
       "      <th>bi_4</th>\n",
       "      <th>key_resp.keys.1</th>\n",
       "      <th>...</th>\n",
       "      <th>key_resp.rt.1</th>\n",
       "      <th>key_resp.rt.2</th>\n",
       "      <th>key_resp.rt.3</th>\n",
       "      <th>key_resp.rt.4</th>\n",
       "      <th>key_resp.rt.5</th>\n",
       "      <th>key_resp.rt.6</th>\n",
       "      <th>key_resp.rt.7</th>\n",
       "      <th>wf_type</th>\n",
       "      <th>meanbf_type</th>\n",
       "      <th>trial_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s311</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>there</td>\n",
       "      <td>threr</td>\n",
       "      <td>th</td>\n",
       "      <td>he</td>\n",
       "      <td>er</td>\n",
       "      <td>re</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716674</td>\n",
       "      <td>0.812385</td>\n",
       "      <td>0.980549</td>\n",
       "      <td>1.932348</td>\n",
       "      <td>2.084488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>highwf</td>\n",
       "      <td>highbf</td>\n",
       "      <td>incorr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s311</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>lucky</td>\n",
       "      <td>lucky</td>\n",
       "      <td>lu</td>\n",
       "      <td>uc</td>\n",
       "      <td>ck</td>\n",
       "      <td>ky</td>\n",
       "      <td>l</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672870</td>\n",
       "      <td>0.784848</td>\n",
       "      <td>0.912876</td>\n",
       "      <td>1.064859</td>\n",
       "      <td>1.192956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>highwf</td>\n",
       "      <td>lowbf</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s311</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>about</td>\n",
       "      <td>about</td>\n",
       "      <td>ab</td>\n",
       "      <td>bo</td>\n",
       "      <td>ou</td>\n",
       "      <td>ut</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>1.012880</td>\n",
       "      <td>1.125029</td>\n",
       "      <td>1.212947</td>\n",
       "      <td>1.301245</td>\n",
       "      <td>1.388791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>highwf</td>\n",
       "      <td>medbf</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s311</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>haole</td>\n",
       "      <td>haole</td>\n",
       "      <td>ha</td>\n",
       "      <td>ao</td>\n",
       "      <td>ol</td>\n",
       "      <td>le</td>\n",
       "      <td>h</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792707</td>\n",
       "      <td>1.032904</td>\n",
       "      <td>1.096603</td>\n",
       "      <td>1.272831</td>\n",
       "      <td>1.352907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>lowwf</td>\n",
       "      <td>medbf</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s311</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>cheer</td>\n",
       "      <td>cheer</td>\n",
       "      <td>ch</td>\n",
       "      <td>he</td>\n",
       "      <td>ee</td>\n",
       "      <td>er</td>\n",
       "      <td>c</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620894</td>\n",
       "      <td>0.732929</td>\n",
       "      <td>0.820822</td>\n",
       "      <td>0.972809</td>\n",
       "      <td>1.028788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>medwf</td>\n",
       "      <td>highbf</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>s311</td>\n",
       "      <td>235</td>\n",
       "      <td>9</td>\n",
       "      <td>edthe</td>\n",
       "      <td>edthe</td>\n",
       "      <td>ed</td>\n",
       "      <td>dt</td>\n",
       "      <td>th</td>\n",
       "      <td>he</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726766</td>\n",
       "      <td>0.814816</td>\n",
       "      <td>0.998677</td>\n",
       "      <td>1.054585</td>\n",
       "      <td>1.110651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pseudo</td>\n",
       "      <td>highbf</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>s311</td>\n",
       "      <td>236</td>\n",
       "      <td>9</td>\n",
       "      <td>pykka</td>\n",
       "      <td>pykka</td>\n",
       "      <td>py</td>\n",
       "      <td>yk</td>\n",
       "      <td>kk</td>\n",
       "      <td>ka</td>\n",
       "      <td>p</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602820</td>\n",
       "      <td>0.674707</td>\n",
       "      <td>0.842636</td>\n",
       "      <td>0.946578</td>\n",
       "      <td>1.018620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pseudo</td>\n",
       "      <td>lowbf</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>s311</td>\n",
       "      <td>237</td>\n",
       "      <td>9</td>\n",
       "      <td>therm</td>\n",
       "      <td>therm</td>\n",
       "      <td>th</td>\n",
       "      <td>he</td>\n",
       "      <td>er</td>\n",
       "      <td>rm</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510682</td>\n",
       "      <td>0.582613</td>\n",
       "      <td>0.638647</td>\n",
       "      <td>0.718633</td>\n",
       "      <td>0.758661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>lowwf</td>\n",
       "      <td>highbf</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>s311</td>\n",
       "      <td>238</td>\n",
       "      <td>9</td>\n",
       "      <td>about</td>\n",
       "      <td>about</td>\n",
       "      <td>ab</td>\n",
       "      <td>bo</td>\n",
       "      <td>ou</td>\n",
       "      <td>ut</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514546</td>\n",
       "      <td>0.618593</td>\n",
       "      <td>0.674578</td>\n",
       "      <td>0.770529</td>\n",
       "      <td>0.810763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>highwf</td>\n",
       "      <td>medbf</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>s311</td>\n",
       "      <td>239</td>\n",
       "      <td>9</td>\n",
       "      <td>druze</td>\n",
       "      <td>druze</td>\n",
       "      <td>dr</td>\n",
       "      <td>ru</td>\n",
       "      <td>uz</td>\n",
       "      <td>ze</td>\n",
       "      <td>d</td>\n",
       "      <td>...</td>\n",
       "      <td>1.158702</td>\n",
       "      <td>1.326544</td>\n",
       "      <td>1.422760</td>\n",
       "      <td>1.526438</td>\n",
       "      <td>1.670625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>lowwf</td>\n",
       "      <td>lowbf</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sID  trial_num  rep_num string resp_string bi_1 bi_2 bi_3 bi_4  \\\n",
       "0    s311          0        0  there       threr   th   he   er   re   \n",
       "1    s311          1        0  lucky       lucky   lu   uc   ck   ky   \n",
       "2    s311          2        0  about       about   ab   bo   ou   ut   \n",
       "3    s311          3        0  haole       haole   ha   ao   ol   le   \n",
       "4    s311          4        0  cheer       cheer   ch   he   ee   er   \n",
       "..    ...        ...      ...    ...         ...  ...  ...  ...  ...   \n",
       "235  s311        235        9  edthe       edthe   ed   dt   th   he   \n",
       "236  s311        236        9  pykka       pykka   py   yk   kk   ka   \n",
       "237  s311        237        9  therm       therm   th   he   er   rm   \n",
       "238  s311        238        9  about       about   ab   bo   ou   ut   \n",
       "239  s311        239        9  druze       druze   dr   ru   uz   ze   \n",
       "\n",
       "    key_resp.keys.1  ... key_resp.rt.1 key_resp.rt.2 key_resp.rt.3  \\\n",
       "0                 t  ...      0.716674      0.812385      0.980549   \n",
       "1                 l  ...      0.672870      0.784848      0.912876   \n",
       "2                 a  ...      1.012880      1.125029      1.212947   \n",
       "3                 h  ...      0.792707      1.032904      1.096603   \n",
       "4                 c  ...      0.620894      0.732929      0.820822   \n",
       "..              ...  ...           ...           ...           ...   \n",
       "235               e  ...      0.726766      0.814816      0.998677   \n",
       "236               p  ...      0.602820      0.674707      0.842636   \n",
       "237               t  ...      0.510682      0.582613      0.638647   \n",
       "238               a  ...      0.514546      0.618593      0.674578   \n",
       "239               d  ...      1.158702      1.326544      1.422760   \n",
       "\n",
       "    key_resp.rt.4 key_resp.rt.5 key_resp.rt.6  key_resp.rt.7  wf_type  \\\n",
       "0        1.932348      2.084488           0.0            0.0   highwf   \n",
       "1        1.064859      1.192956           0.0            0.0   highwf   \n",
       "2        1.301245      1.388791           0.0            0.0   highwf   \n",
       "3        1.272831      1.352907           0.0            0.0    lowwf   \n",
       "4        0.972809      1.028788           0.0            0.0    medwf   \n",
       "..            ...           ...           ...            ...      ...   \n",
       "235      1.054585      1.110651           0.0            0.0   pseudo   \n",
       "236      0.946578      1.018620           0.0            0.0   pseudo   \n",
       "237      0.718633      0.758661           0.0            0.0    lowwf   \n",
       "238      0.770529      0.810763           0.0            0.0   highwf   \n",
       "239      1.526438      1.670625           0.0            0.0    lowwf   \n",
       "\n",
       "     meanbf_type  trial_corr  \n",
       "0         highbf      incorr  \n",
       "1          lowbf        corr  \n",
       "2          medbf        corr  \n",
       "3          medbf        corr  \n",
       "4         highbf        corr  \n",
       "..           ...         ...  \n",
       "235       highbf        corr  \n",
       "236        lowbf        corr  \n",
       "237       highbf        corr  \n",
       "238        medbf        corr  \n",
       "239        lowbf        corr  \n",
       "\n",
       "[240 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can also be specified\n",
    "#     print(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sID</th>\n",
       "      <th>bigram_num</th>\n",
       "      <th>trial_num</th>\n",
       "      <th>rep_num</th>\n",
       "      <th>bigram_loc</th>\n",
       "      <th>bigram</th>\n",
       "      <th>resp_bigram</th>\n",
       "      <th>IKI</th>\n",
       "      <th>string</th>\n",
       "      <th>resp_string</th>\n",
       "      <th>bg_freq</th>\n",
       "      <th>bf_type</th>\n",
       "      <th>meanbf_type</th>\n",
       "      <th>wf_type</th>\n",
       "      <th>trial_corr</th>\n",
       "      <th>bg_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s311</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>th</td>\n",
       "      <td>th</td>\n",
       "      <td>0.095711</td>\n",
       "      <td>there</td>\n",
       "      <td>threr</td>\n",
       "      <td>22288309.0</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>incorr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s311</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>he</td>\n",
       "      <td>hr</td>\n",
       "      <td>0.168164</td>\n",
       "      <td>there</td>\n",
       "      <td>threr</td>\n",
       "      <td>21484684.0</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>incorr</td>\n",
       "      <td>incorr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s311</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>er</td>\n",
       "      <td>re</td>\n",
       "      <td>0.951799</td>\n",
       "      <td>there</td>\n",
       "      <td>threr</td>\n",
       "      <td>12934901.0</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>incorr</td>\n",
       "      <td>incorr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s311</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>re</td>\n",
       "      <td>er</td>\n",
       "      <td>0.152140</td>\n",
       "      <td>there</td>\n",
       "      <td>threr</td>\n",
       "      <td>10687711.0</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>incorr</td>\n",
       "      <td>incorr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s311</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>lu</td>\n",
       "      <td>lu</td>\n",
       "      <td>0.111978</td>\n",
       "      <td>lucky</td>\n",
       "      <td>lucky</td>\n",
       "      <td>568081.0</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>s311</td>\n",
       "      <td>966</td>\n",
       "      <td>238</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>ut</td>\n",
       "      <td>ut</td>\n",
       "      <td>0.040234</td>\n",
       "      <td>about</td>\n",
       "      <td>about</td>\n",
       "      <td>3257233.0</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>high</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>s311</td>\n",
       "      <td>967</td>\n",
       "      <td>239</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>dr</td>\n",
       "      <td>dr</td>\n",
       "      <td>0.167842</td>\n",
       "      <td>druze</td>\n",
       "      <td>druze</td>\n",
       "      <td>567936.0</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>s311</td>\n",
       "      <td>968</td>\n",
       "      <td>239</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>ru</td>\n",
       "      <td>ru</td>\n",
       "      <td>0.096216</td>\n",
       "      <td>druze</td>\n",
       "      <td>druze</td>\n",
       "      <td>705492.0</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>s311</td>\n",
       "      <td>969</td>\n",
       "      <td>239</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>uz</td>\n",
       "      <td>uz</td>\n",
       "      <td>0.103678</td>\n",
       "      <td>druze</td>\n",
       "      <td>druze</td>\n",
       "      <td>19836.0</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>s311</td>\n",
       "      <td>970</td>\n",
       "      <td>239</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>ze</td>\n",
       "      <td>ze</td>\n",
       "      <td>0.144187</td>\n",
       "      <td>druze</td>\n",
       "      <td>druze</td>\n",
       "      <td>210896.0</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>971 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sID  bigram_num  trial_num  rep_num  bigram_loc bigram resp_bigram  \\\n",
       "0    s311           0          0        0           0     th          th   \n",
       "1    s311           1          0        0           1     he          hr   \n",
       "2    s311           2          0        0           2     er          re   \n",
       "3    s311           3          0        0           3     re          er   \n",
       "4    s311           4          1        0           0     lu          lu   \n",
       "..    ...         ...        ...      ...         ...    ...         ...   \n",
       "966  s311         966        238        9           3     ut          ut   \n",
       "967  s311         967        239        9           0     dr          dr   \n",
       "968  s311         968        239        9           1     ru          ru   \n",
       "969  s311         969        239        9           2     uz          uz   \n",
       "970  s311         970        239        9           3     ze          ze   \n",
       "\n",
       "          IKI string resp_string     bg_freq bf_type meanbf_type wf_type  \\\n",
       "0    0.095711  there       threr  22288309.0    high        high    high   \n",
       "1    0.168164  there       threr  21484684.0    high        high    high   \n",
       "2    0.951799  there       threr  12934901.0    high        high    high   \n",
       "3    0.152140  there       threr  10687711.0    high        high    high   \n",
       "4    0.111978  lucky       lucky    568081.0     med         low    high   \n",
       "..        ...    ...         ...         ...     ...         ...     ...   \n",
       "966  0.040234  about       about   3257233.0     med         med    high   \n",
       "967  0.167842  druze       druze    567936.0     med         low     low   \n",
       "968  0.096216  druze       druze    705492.0     med         low     low   \n",
       "969  0.103678  druze       druze     19836.0     low         low     low   \n",
       "970  0.144187  druze       druze    210896.0     low         low     low   \n",
       "\n",
       "    trial_corr bg_corr  \n",
       "0       incorr    corr  \n",
       "1       incorr  incorr  \n",
       "2       incorr  incorr  \n",
       "3       incorr  incorr  \n",
       "4         corr    corr  \n",
       "..         ...     ...  \n",
       "966       corr    corr  \n",
       "967       corr    corr  \n",
       "968       corr    corr  \n",
       "969       corr    corr  \n",
       "970       corr    corr  \n",
       "\n",
       "[971 rows x 16 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_df\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can also be specified\n",
    "#     print(bigram_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
