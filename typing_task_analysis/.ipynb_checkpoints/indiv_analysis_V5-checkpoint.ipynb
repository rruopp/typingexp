{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import variation\n",
    "import glob\n",
    "import os\n",
    "import typingmod as typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## mounting to ION server\n",
    "# os.system(\"osascript -e 'mount volume \\\"smb://ion-nas.uoregon.edu\\\" \\\n",
    "#           as user name \\\"greenhouse\\\" with password \\\"password\\\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining function to organize bigrams into rows\n",
    "def bigram_byrow():\n",
    "    bigrams = []\n",
    "    for index, row in keys_intocolumns.iterrows():\n",
    "        for column in range(0, (len(keys_intocolumns.columns) - 1)):\n",
    "            if (keys_intocolumns[column][index] != None and float('nan')) and (keys_intocolumns[column + 1][index] != None and float('nan')):\n",
    "                bigram = (keys_intocolumns[column][index] + keys_intocolumns[column + 1][index])\n",
    "                bigram = (bigram.replace(\"'\", \"\")).replace(\" \", \"\")\n",
    "                iki = (main_df['key_resp.rt.%(second)d' % {'second':  column + 2 }][index] - main_df['key_resp.rt.%(first)d' % { 'first': column +1 }][index])\n",
    "                bigrams.append([index, column, bigram, iki, main_df['string'][index], main_df['resp_string'][index]])\n",
    "    return(bigrams)\n",
    "\n",
    "## defining function that separates words in to bigrams\n",
    "def bi_byword(word):\n",
    "    bi_results = []\n",
    "    for y in range(0, (len(word)-1)):\n",
    "        bigram = word[y] + word[y+1]\n",
    "        bi_results.append(bigram)\n",
    "    return bi_results\n",
    "\n",
    "## defining function that separates all words into bigrams\n",
    "def bi_allwords():\n",
    "    bigrams = []\n",
    "    for word in df['string']:\n",
    "        bigrams.append(bi_byword(word))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/greenhouse/typingtask_data/subject_data/s262_01232024/psychopy_data/edited/s262_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s262_01232024/psychopy_data/edited/s262_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s261_12122023/psychopy_data/edited/s261_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s261_12122023/psychopy_data/edited/s261_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s240_11162023/psychopy_data/edited/s240_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s240_11162023/psychopy_data/edited/s240_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s217_11092023/psychopy_data/edited/s217_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s217_11092023/psychopy_data/edited/s217_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s176_10262023/psychopy_data/edited/s176_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s176_10262023/psychopy_data/edited/s176_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s20_02082024/psychopy_data/edited/s20_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s20_02082024/psychopy_data/edited/s20_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s267_02122024/psychopy_data/edited/s267_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s267_02122024/psychopy_data/edited/s267_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s263_01312024/psychopy_data/edited/s263_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s263_01312024/psychopy_data/edited/s263_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s254_11022023/psychopy_data/edited/s254_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s254_11022023/psychopy_data/edited/s254_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s09_02162024/psychopy_data/edited/s09_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s09_02162024/psychopy_data/edited/s09_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s209_09202023/psychopy_data/edited/s209_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s209_09202023/psychopy_data/edited/s209_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s276_02212024/psychopy_data/edited/s276_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s276_02212024/psychopy_data/edited/s276_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s278_02232024/psychopy_data/edited/s278_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s278_02232024/psychopy_data/edited/s278_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s279_02232024/psychopy_data/edited/s279_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s279_02232024/psychopy_data/edited/s279_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s282_02292024/psychopy_data/edited/s282_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s282_02292024/psychopy_data/edited/s282_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s283_02292024/psychopy_data/edited/s283_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s283_02292024/psychopy_data/edited/s283_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s286_03082024/psychopy_data/edited/s286_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s286_03082024/psychopy_data/edited/s286_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s302_05282024/psychopy_data/edited/s302_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s302_05282024/psychopy_data/edited/s302_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s304_07102024/psychopy_data/edited/s304_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s304_07102024/psychopy_data/edited/s304_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s306_07122024/psychopy_data/edited/s306_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s306_07122024/psychopy_data/edited/s306_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s305_07112024/psychopy_data/edited/s305_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s305_07112024/psychopy_data/edited/s305_bybigram.csv\n"
     ]
    }
   ],
   "source": [
    "## create dataframes tiral-based and bigram-based dataframes for each subject ##\n",
    "\n",
    "## importing experiment data\n",
    "server = r'/Volumes/greenhouse/typingtask_data/subject_data'\n",
    "server_noturbo = r'/Volumes/greenhouse/typingtask_data/subject_data/not_used/no_turbotyping/'\n",
    "os.chdir(server)\n",
    "folders = os.listdir()\n",
    "\n",
    "# looping through subjects\n",
    "sub_folders = list(filter(lambda x: x.startswith('s', 0, 1), folders))\n",
    "for sub in sub_folders:\n",
    "    sub_folder = r'/Volumes/greenhouse/typingtask_data/subject_data/%s/psychopy_data/' % sub\n",
    "    os.chdir(sub_folder)\n",
    "    sID = sub.split('_', 1)[0]\n",
    "    og_df = pd.read_csv(glob.glob('*.csv')[0])   \n",
    "\n",
    "## filters through subjects without turbotyping data\n",
    "# sub_folders = list(filter(lambda x: x.startswith('s', 0, 1), folders))\n",
    "# for sub in sub_folders:\n",
    "#     sub_folder = server_noturbo + r'%s/psychopy_data/' % sub\n",
    "#     os.chdir(sub_folder)\n",
    "#     sID = sub.split('_', 1)[0]\n",
    "#     og_df = pd.read_csv(glob.glob('*.csv')[0])  \n",
    "   \n",
    "    ## deleting first 3 practice trials -- EDIT FOR ANY TRIALS YOU WANT TO IMMEDIATELY EXCLUDE\n",
    "    df = (og_df.drop(labels=[0, 1, 2], axis=0)).reset_index(drop = True) \n",
    "    \n",
    "    ## expanding nested key_resp.rt values into separate columns, making new dataframe, and turning values back into floats from strings\n",
    "    stripped_rts_1 = ((df['key_resp_1.rt'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    stripped_rts_2 = ((df['key_resp_2.rt'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    rts_intocolumns = (pd.concat([stripped_rts_1, stripped_rts_2])).reset_index(drop = True)\n",
    "    \n",
    "    ## renames rt columns to automatically match dataset\n",
    "    DF = rts_intocolumns\n",
    "    renamed_rt = DF.rename(columns = { 0:'key_resp.rt.%s' %(0+1) })\n",
    "    for n in range(0, len(DF.columns)):\n",
    "        renamed_rt = renamed_rt.rename(columns = { n:'key_resp.rt.%s' %(n+1) })\n",
    "    expanded_rts = renamed_rt.astype(float).fillna(0) ##replacing NaNs with zeroes\n",
    "\n",
    "    ## expanding nested key_resp.keys values into separate columns and making new dataframe\n",
    "    stripped_keys_1 = ((df['key_resp_1.keys'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    stripped_keys_2 = ((df['key_resp_2.keys'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    keys_intocolumns = (pd.concat([stripped_keys_1, stripped_keys_2])).reset_index(drop = True)\n",
    "    keys_intocolumns = keys_intocolumns.where(pd.notnull(keys_intocolumns), None) \n",
    "        # ^ also replaces any added NaNs with Nones\n",
    "\n",
    "    ## renames key columns to automatically match dataset\n",
    "    DF = keys_intocolumns\n",
    "    expanded_keys = DF.rename(columns = { 0:'key_resp.keys.%s' %(0+1) })\n",
    "    for n in range(0, len(DF.columns)):\n",
    "        expanded_keys = expanded_keys.rename(columns = { n:'key_resp.keys.%s' %(n+1) })\n",
    "\n",
    "    ## getting rid of apostrophes and spaces in key values\n",
    "    cols_to_change = (expanded_keys.iloc[:, 0:])\n",
    "    for col in cols_to_change:\n",
    "        expanded_keys[col] = expanded_keys[col].str.replace(\"'\", \"\")\n",
    "        expanded_keys[col] = expanded_keys[col].str.replace(\" \", \"\")\n",
    "\n",
    "    ## combining key_resp.keys into one simple string to easily represent typed responses\n",
    "    responses_1 = pd.DataFrame((df['key_resp_1.keys'].str.replace(\"[', ]\", \"\", regex=True).str.strip(\"[]\")).dropna()).rename(columns = {'key_resp_1.keys':'resp_string'})\n",
    "    responses_2 = pd.DataFrame((df['key_resp_2.keys'].str.replace(\"[', ]\", \"\", regex=True).str.strip(\"[]\")).dropna()).rename(columns = {'key_resp_2.keys':'resp_string'})\n",
    "    responses = (pd.concat([responses_1, responses_2])).reset_index(drop = True)\n",
    "\n",
    "    ## identifying bigrams in words to add to larger dataframe\n",
    "    task_bigrams = pd.DataFrame(bi_allwords())\n",
    "    task_bigrams.columns = ['bi_1', 'bi_2', 'bi_3', 'bi_4']\n",
    "    \n",
    "    ## combining expanded rt, expanded keys, and response string values with column for strings typed each trial to create more useful dataframe\n",
    "    ## (does not have all the random timing data of other events occuring during the task)\n",
    "    main_df = pd.concat([responses, task_bigrams, expanded_keys, expanded_rts], axis = 1)\n",
    "    main_df.insert(0, 'string', df['string'], True)\n",
    "\n",
    "    ## creating column for WF type for each trial\n",
    "    main_df['wf_type'] = \"\"\n",
    "    for index, data in main_df.iterrows():\n",
    "        if main_df.loc[index, 'string'] in typ.highwf:\n",
    "            main_df.loc[index, 'wf_type'] = 'highwf'\n",
    "        if main_df.loc[index, 'string'] in typ.medwf:\n",
    "            main_df.loc[index, 'wf_type'] = 'medwf'\n",
    "        if main_df.loc[index, 'string'] in typ.lowwf:\n",
    "            main_df.loc[index, 'wf_type'] = 'lowwf'\n",
    "        if main_df.loc[index, 'string'] in typ.pseudo:\n",
    "            main_df.loc[index, 'wf_type'] = 'pseudo'\n",
    "\n",
    "    ## creating column for BF type for each trial\n",
    "    main_df['meanbf_type'] = \"\"\n",
    "    for index, data in main_df.iterrows():\n",
    "        if main_df.loc[index, 'string'] in typ.avg_highbf:\n",
    "            main_df.loc[index, 'meanbf_type'] = 'highbf'\n",
    "        if main_df.loc[index, 'string'] in typ.avg_medbf:\n",
    "            main_df.loc[index, 'meanbf_type'] = 'medbf'\n",
    "        if main_df.loc[index, 'string'] in typ.avg_lowbf:\n",
    "            main_df.loc[index, 'meanbf_type'] = 'lowbf'\n",
    "\n",
    "    ## creating column for trial (useful for group analysis)\n",
    "    trial_nums = []\n",
    "    for index, data in main_df.iterrows():\n",
    "        trial_nums.append(index)\n",
    "    main_df.insert(0, 'trial_num', trial_nums)\n",
    "\n",
    "    ## creating column for subject ID (also useful for group analysis)\n",
    "    main_ID = [sID]*len(main_df)\n",
    "    main_df.insert(0, 'sID', main_ID)\n",
    "\n",
    "    ## creating columns for word repetition number\n",
    "    main_df.insert(2, 'rep_num', '')\n",
    "    main_df['rep_num'] = main_df.groupby(['sID', 'string']).cumcount()\n",
    "\n",
    "    ## creating column for if trial is correct or not\n",
    "    main_df['trial_corr'] = ''\n",
    "    corr_trials = (main_df[main_df.string \n",
    "                   == main_df.resp_string])\n",
    "    corr_indices = list(corr_trials.index.values)\n",
    "    main_df.loc[corr_indices, 'trial_corr'] = \"corr\"\n",
    "    \n",
    "    incorr_trials = (main_df[main_df.string \n",
    "                     != main_df.resp_string])\n",
    "    incorr_indices = list(incorr_trials.index.values)\n",
    "    main_df.loc[incorr_indices, 'trial_corr'] = \"incorr\"\n",
    "    \n",
    "    ## making csv from dataframe\n",
    "    edited_path = os.path.join(sub_folder, 'edited')\n",
    "    if os.path.exists(edited_path) == False:\n",
    "        os.mkdir(edited_path)\n",
    "    bytrial_path = os.path.join(edited_path, '%s_bytrial.csv' % sID)\n",
    "    print(bytrial_path)\n",
    "    main_df.to_csv(bytrial_path)\n",
    "\n",
    "    \n",
    "    ## BIGRAM DATAFRAME ##\n",
    "    bigram_df = (pd.DataFrame(bigram_byrow())).rename(columns={0: \"trial_num\", 1: \"bigram_loc\",  2:\"resp_bigram\", 3: \"IKI\", 4: \"string\", 5: \"resp_string\"})\n",
    "\n",
    "    ## creating column for bigram # (useful for group analysis)\n",
    "    bigram_nums = []\n",
    "    for index, data in bigram_df.iterrows():\n",
    "        bigram_nums.append(index)\n",
    "    bigram_df.insert(0, 'bigram_num', bigram_nums)\n",
    "\n",
    "    ## creating column for subject ID (also useful for group analysis)\n",
    "    bigram_ID = [sID]*len(bigram_df)\n",
    "    bigram_df.insert(0, 'sID', bigram_ID)\n",
    "\n",
    "    ## creating column for correct bigram (as opposed to the typed bigram)\n",
    "    bigram_df.insert(4, 'bigram', '')\n",
    "    for index, row in bigram_df.iterrows():\n",
    "        loc = bigram_df.loc[index, 'bigram_loc']\n",
    "        loc_list = [0, 1, 2, 3]\n",
    "        if loc in loc_list:\n",
    "            corr = bi_byword(bigram_df.loc[index, 'string'])[loc]\n",
    "        else:\n",
    "            corr = ''\n",
    "        bigram_df.loc[index, 'bigram'] = corr\n",
    "\n",
    "    ## creating column for rep #\n",
    "    bigram_df.insert(3, 'rep_num', '')\n",
    "    bigram_df['rep_num'] = bigram_df.groupby(['sID', 'string', 'bigram']).cumcount()\n",
    "    \n",
    "    ## creating column for bigram frequency\n",
    "    bg_freqs = pd.read_csv(r'/Users/rubi/Desktop/Github/typingexp/typing_task_analysis/bg_freqs.csv') ## EDIT TO MAKE USEFUL ON OTHER COMPUTERS\n",
    "    bg_freqs.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "    freq_dict = bg_freqs.set_index('Bigrams')['Frequency'].to_dict()\n",
    "    bigram_df['bg_freq'] = bigram_df['bigram'].map(freq_dict)\n",
    "\n",
    "    ## creating column for bigram type\n",
    "    name_list = ['high', 'med', 'low', 'pseudo']\n",
    "\n",
    "    for index, bf_type in enumerate(typ.bf_types):\n",
    "        by_bf = bigram_df[bigram_df.bigram.isin(bf_type)]\n",
    "        rows = by_bf.index\n",
    "        bigram_df.loc[rows, 'bf_type'] = name_list[index]\n",
    "\n",
    "    ## creating a column for mean bigram type\n",
    "    for index, avgbf_type in enumerate(typ.avgbf_types):\n",
    "        by_bf = bigram_df[bigram_df.string.isin(avgbf_type)]\n",
    "        rows = by_bf.index\n",
    "        bigram_df.loc[rows, 'meanbf_type'] = name_list[index]\n",
    "\n",
    "    ## creating a column for mean bigram type\n",
    "    for index, wf_type in enumerate(typ.wf_types):\n",
    "        by_wf = bigram_df[bigram_df.string.isin(wf_type)]\n",
    "        rows = by_wf.index\n",
    "        bigram_df.loc[rows, 'wf_type'] = name_list[index]\n",
    "\n",
    "    ## creating column for if trial is correct or not\n",
    "    bigram_df['trial_corr'] = ''\n",
    "    corr_trials_bybg = (bigram_df[bigram_df.string \n",
    "                   == bigram_df.resp_string])\n",
    "    corr_indices_bybg = list(corr_trials_bybg.index.values)\n",
    "    bigram_df.loc[corr_indices_bybg, 'trial_corr'] = \"corr\"\n",
    "    \n",
    "    incorr_trials_bybg = (bigram_df[bigram_df.string \n",
    "                     != bigram_df.resp_string])\n",
    "    incorr_indices_bybg = list(incorr_trials_bybg.index.values)\n",
    "    bigram_df.loc[incorr_indices_bybg, 'trial_corr'] = \"incorr\"\n",
    "\n",
    "    ## creating column for if bigram is correct or not\n",
    "    bigram_df['bg_corr'] = ''\n",
    "    corr_bgs = (bigram_df[bigram_df.bigram \n",
    "                   == bigram_df.resp_bigram])\n",
    "    corr_bg_indices = list(corr_bgs.index.values)\n",
    "    bigram_df.loc[corr_bg_indices, 'bg_corr'] = \"corr\"\n",
    "    \n",
    "    incorr_bgs = (bigram_df[bigram_df.bigram \n",
    "                     != bigram_df.resp_bigram])\n",
    "    incorr_bg_indices = list(incorr_bgs.index.values)\n",
    "    bigram_df.loc[incorr_bg_indices, 'bg_corr'] = \"incorr\"\n",
    "\n",
    "    ## making csv from dataframe\n",
    "    bybigram_path = os.path.join(edited_path, '%s_bybigram.csv' % sID)\n",
    "    print(bybigram_path)\n",
    "    bigram_df.to_csv(bybigram_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sID</th>\n",
       "      <th>trial_num</th>\n",
       "      <th>rep_num</th>\n",
       "      <th>string</th>\n",
       "      <th>resp_string</th>\n",
       "      <th>bi_1</th>\n",
       "      <th>bi_2</th>\n",
       "      <th>bi_3</th>\n",
       "      <th>bi_4</th>\n",
       "      <th>key_resp.keys.1</th>\n",
       "      <th>...</th>\n",
       "      <th>key_resp.rt.1</th>\n",
       "      <th>key_resp.rt.2</th>\n",
       "      <th>key_resp.rt.3</th>\n",
       "      <th>key_resp.rt.4</th>\n",
       "      <th>key_resp.rt.5</th>\n",
       "      <th>key_resp.rt.6</th>\n",
       "      <th>wf_type</th>\n",
       "      <th>meanbf_type</th>\n",
       "      <th>trial_corr</th>\n",
       "      <th>corr_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s305</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>think</td>\n",
       "      <td>think</td>\n",
       "      <td>th</td>\n",
       "      <td>hi</td>\n",
       "      <td>in</td>\n",
       "      <td>nk</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777044</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>1.113209</td>\n",
       "      <td>1.273128</td>\n",
       "      <td>1.417325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>highwf</td>\n",
       "      <td>highbf</td>\n",
       "      <td></td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s305</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>haole</td>\n",
       "      <td>haole</td>\n",
       "      <td>ha</td>\n",
       "      <td>ao</td>\n",
       "      <td>ol</td>\n",
       "      <td>le</td>\n",
       "      <td>h</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973728</td>\n",
       "      <td>1.165631</td>\n",
       "      <td>1.333673</td>\n",
       "      <td>1.533691</td>\n",
       "      <td>1.654069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>lowwf</td>\n",
       "      <td>medbf</td>\n",
       "      <td></td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s305</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>edthe</td>\n",
       "      <td>edthe</td>\n",
       "      <td>ed</td>\n",
       "      <td>dt</td>\n",
       "      <td>th</td>\n",
       "      <td>he</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913808</td>\n",
       "      <td>1.105976</td>\n",
       "      <td>1.281843</td>\n",
       "      <td>1.417973</td>\n",
       "      <td>1.521869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pseudo</td>\n",
       "      <td>highbf</td>\n",
       "      <td></td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s305</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>belly</td>\n",
       "      <td>belly</td>\n",
       "      <td>be</td>\n",
       "      <td>el</td>\n",
       "      <td>ll</td>\n",
       "      <td>ly</td>\n",
       "      <td>b</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661679</td>\n",
       "      <td>0.837608</td>\n",
       "      <td>1.005634</td>\n",
       "      <td>1.141670</td>\n",
       "      <td>1.605719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>medwf</td>\n",
       "      <td>medbf</td>\n",
       "      <td></td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s305</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>cheer</td>\n",
       "      <td>cheer</td>\n",
       "      <td>ch</td>\n",
       "      <td>he</td>\n",
       "      <td>ee</td>\n",
       "      <td>er</td>\n",
       "      <td>c</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626029</td>\n",
       "      <td>0.753881</td>\n",
       "      <td>0.857857</td>\n",
       "      <td>1.001862</td>\n",
       "      <td>1.121864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>medwf</td>\n",
       "      <td>highbf</td>\n",
       "      <td></td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>s305</td>\n",
       "      <td>235</td>\n",
       "      <td>9</td>\n",
       "      <td>cheer</td>\n",
       "      <td>cheer</td>\n",
       "      <td>ch</td>\n",
       "      <td>he</td>\n",
       "      <td>ee</td>\n",
       "      <td>er</td>\n",
       "      <td>c</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612286</td>\n",
       "      <td>0.740279</td>\n",
       "      <td>0.804288</td>\n",
       "      <td>0.956597</td>\n",
       "      <td>1.068277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>medwf</td>\n",
       "      <td>highbf</td>\n",
       "      <td></td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>s305</td>\n",
       "      <td>236</td>\n",
       "      <td>9</td>\n",
       "      <td>haole</td>\n",
       "      <td>haole</td>\n",
       "      <td>ha</td>\n",
       "      <td>ao</td>\n",
       "      <td>ol</td>\n",
       "      <td>le</td>\n",
       "      <td>h</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.608155</td>\n",
       "      <td>0.689170</td>\n",
       "      <td>0.864152</td>\n",
       "      <td>0.976113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>lowwf</td>\n",
       "      <td>medbf</td>\n",
       "      <td></td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>s305</td>\n",
       "      <td>237</td>\n",
       "      <td>9</td>\n",
       "      <td>theme</td>\n",
       "      <td>theme</td>\n",
       "      <td>th</td>\n",
       "      <td>he</td>\n",
       "      <td>em</td>\n",
       "      <td>me</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500097</td>\n",
       "      <td>0.588131</td>\n",
       "      <td>0.684550</td>\n",
       "      <td>0.796235</td>\n",
       "      <td>0.852109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>medwf</td>\n",
       "      <td>highbf</td>\n",
       "      <td></td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>s305</td>\n",
       "      <td>238</td>\n",
       "      <td>9</td>\n",
       "      <td>about</td>\n",
       "      <td>about</td>\n",
       "      <td>ab</td>\n",
       "      <td>bo</td>\n",
       "      <td>ou</td>\n",
       "      <td>ut</td>\n",
       "      <td>a</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.632218</td>\n",
       "      <td>0.784196</td>\n",
       "      <td>0.888196</td>\n",
       "      <td>1.032283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>highwf</td>\n",
       "      <td>medbf</td>\n",
       "      <td></td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>s305</td>\n",
       "      <td>239</td>\n",
       "      <td>9</td>\n",
       "      <td>lucky</td>\n",
       "      <td>lucky</td>\n",
       "      <td>lu</td>\n",
       "      <td>uc</td>\n",
       "      <td>ck</td>\n",
       "      <td>ky</td>\n",
       "      <td>l</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468866</td>\n",
       "      <td>0.620001</td>\n",
       "      <td>0.772135</td>\n",
       "      <td>0.876027</td>\n",
       "      <td>1.020023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>highwf</td>\n",
       "      <td>lowbf</td>\n",
       "      <td></td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sID  trial_num  rep_num string resp_string bi_1 bi_2 bi_3 bi_4  \\\n",
       "0    s305          0        0  think       think   th   hi   in   nk   \n",
       "1    s305          1        0  haole       haole   ha   ao   ol   le   \n",
       "2    s305          2        0  edthe       edthe   ed   dt   th   he   \n",
       "3    s305          3        0  belly       belly   be   el   ll   ly   \n",
       "4    s305          4        0  cheer       cheer   ch   he   ee   er   \n",
       "..    ...        ...      ...    ...         ...  ...  ...  ...  ...   \n",
       "235  s305        235        9  cheer       cheer   ch   he   ee   er   \n",
       "236  s305        236        9  haole       haole   ha   ao   ol   le   \n",
       "237  s305        237        9  theme       theme   th   he   em   me   \n",
       "238  s305        238        9  about       about   ab   bo   ou   ut   \n",
       "239  s305        239        9  lucky       lucky   lu   uc   ck   ky   \n",
       "\n",
       "    key_resp.keys.1  ... key_resp.rt.1 key_resp.rt.2 key_resp.rt.3  \\\n",
       "0                 t  ...      0.777044      1.009041      1.113209   \n",
       "1                 h  ...      0.973728      1.165631      1.333673   \n",
       "2                 e  ...      0.913808      1.105976      1.281843   \n",
       "3                 b  ...      0.661679      0.837608      1.005634   \n",
       "4                 c  ...      0.626029      0.753881      0.857857   \n",
       "..              ...  ...           ...           ...           ...   \n",
       "235               c  ...      0.612286      0.740279      0.804288   \n",
       "236               h  ...      0.512195      0.608155      0.689170   \n",
       "237               t  ...      0.500097      0.588131      0.684550   \n",
       "238               a  ...      0.512195      0.632218      0.784196   \n",
       "239               l  ...      0.468866      0.620001      0.772135   \n",
       "\n",
       "    key_resp.rt.4 key_resp.rt.5  key_resp.rt.6  wf_type  meanbf_type  \\\n",
       "0        1.273128      1.417325            0.0   highwf       highbf   \n",
       "1        1.533691      1.654069            0.0    lowwf        medbf   \n",
       "2        1.417973      1.521869            0.0   pseudo       highbf   \n",
       "3        1.141670      1.605719            0.0    medwf        medbf   \n",
       "4        1.001862      1.121864            0.0    medwf       highbf   \n",
       "..            ...           ...            ...      ...          ...   \n",
       "235      0.956597      1.068277            0.0    medwf       highbf   \n",
       "236      0.864152      0.976113            0.0    lowwf        medbf   \n",
       "237      0.796235      0.852109            0.0    medwf       highbf   \n",
       "238      0.888196      1.032283            0.0   highwf        medbf   \n",
       "239      0.876027      1.020023            0.0   highwf        lowbf   \n",
       "\n",
       "     trial_corr  corr_type  \n",
       "0                     corr  \n",
       "1                     corr  \n",
       "2                     corr  \n",
       "3                     corr  \n",
       "4                     corr  \n",
       "..          ...        ...  \n",
       "235                   corr  \n",
       "236                   corr  \n",
       "237                   corr  \n",
       "238                   corr  \n",
       "239                   corr  \n",
       "\n",
       "[240 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can also be specified\n",
    "#     print(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sID</th>\n",
       "      <th>bigram_num</th>\n",
       "      <th>trial_num</th>\n",
       "      <th>rep_num</th>\n",
       "      <th>bigram_loc</th>\n",
       "      <th>bigram</th>\n",
       "      <th>resp_bigram</th>\n",
       "      <th>IKI</th>\n",
       "      <th>string</th>\n",
       "      <th>resp_string</th>\n",
       "      <th>bg_freq</th>\n",
       "      <th>bf_type</th>\n",
       "      <th>meanbf_type</th>\n",
       "      <th>wf_type</th>\n",
       "      <th>trial_corr</th>\n",
       "      <th>bg_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s305</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>th</td>\n",
       "      <td>th</td>\n",
       "      <td>0.231997</td>\n",
       "      <td>think</td>\n",
       "      <td>think</td>\n",
       "      <td>22288309.0</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s305</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>0.104168</td>\n",
       "      <td>think</td>\n",
       "      <td>think</td>\n",
       "      <td>6198006.0</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s305</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>0.159919</td>\n",
       "      <td>think</td>\n",
       "      <td>think</td>\n",
       "      <td>13597302.0</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s305</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>nk</td>\n",
       "      <td>nk</td>\n",
       "      <td>0.144196</td>\n",
       "      <td>think</td>\n",
       "      <td>think</td>\n",
       "      <td>445067.0</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s305</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ha</td>\n",
       "      <td>ha</td>\n",
       "      <td>0.191903</td>\n",
       "      <td>haole</td>\n",
       "      <td>haole</td>\n",
       "      <td>6967591.0</td>\n",
       "      <td>high</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>s305</td>\n",
       "      <td>953</td>\n",
       "      <td>238</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>ut</td>\n",
       "      <td>ut</td>\n",
       "      <td>0.144087</td>\n",
       "      <td>about</td>\n",
       "      <td>about</td>\n",
       "      <td>3257233.0</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>high</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>s305</td>\n",
       "      <td>954</td>\n",
       "      <td>239</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>lu</td>\n",
       "      <td>lu</td>\n",
       "      <td>0.151135</td>\n",
       "      <td>lucky</td>\n",
       "      <td>lucky</td>\n",
       "      <td>568081.0</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>s305</td>\n",
       "      <td>955</td>\n",
       "      <td>239</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>uc</td>\n",
       "      <td>uc</td>\n",
       "      <td>0.152134</td>\n",
       "      <td>lucky</td>\n",
       "      <td>lucky</td>\n",
       "      <td>891233.0</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>s305</td>\n",
       "      <td>956</td>\n",
       "      <td>239</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>ck</td>\n",
       "      <td>ck</td>\n",
       "      <td>0.103892</td>\n",
       "      <td>lucky</td>\n",
       "      <td>lucky</td>\n",
       "      <td>925655.0</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>s305</td>\n",
       "      <td>957</td>\n",
       "      <td>239</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>ky</td>\n",
       "      <td>ky</td>\n",
       "      <td>0.143996</td>\n",
       "      <td>lucky</td>\n",
       "      <td>lucky</td>\n",
       "      <td>49764.0</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>958 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sID  bigram_num  trial_num  rep_num  bigram_loc bigram resp_bigram  \\\n",
       "0    s305           0          0        0           0     th          th   \n",
       "1    s305           1          0        0           1     hi          hi   \n",
       "2    s305           2          0        0           2     in          in   \n",
       "3    s305           3          0        0           3     nk          nk   \n",
       "4    s305           4          1        0           0     ha          ha   \n",
       "..    ...         ...        ...      ...         ...    ...         ...   \n",
       "953  s305         953        238        9           3     ut          ut   \n",
       "954  s305         954        239        9           0     lu          lu   \n",
       "955  s305         955        239        9           1     uc          uc   \n",
       "956  s305         956        239        9           2     ck          ck   \n",
       "957  s305         957        239        9           3     ky          ky   \n",
       "\n",
       "          IKI string resp_string     bg_freq bf_type meanbf_type wf_type  \\\n",
       "0    0.231997  think       think  22288309.0    high        high    high   \n",
       "1    0.104168  think       think   6198006.0    high        high    high   \n",
       "2    0.159919  think       think  13597302.0    high        high    high   \n",
       "3    0.144196  think       think    445067.0     low        high    high   \n",
       "4    0.191903  haole       haole   6967591.0    high         med     low   \n",
       "..        ...    ...         ...         ...     ...         ...     ...   \n",
       "953  0.144087  about       about   3257233.0     med         med    high   \n",
       "954  0.151135  lucky       lucky    568081.0     med         low    high   \n",
       "955  0.152134  lucky       lucky    891233.0     med         low    high   \n",
       "956  0.103892  lucky       lucky    925655.0     med         low    high   \n",
       "957  0.143996  lucky       lucky     49764.0     low         low    high   \n",
       "\n",
       "    trial_corr bg_corr  \n",
       "0         corr    corr  \n",
       "1         corr    corr  \n",
       "2         corr    corr  \n",
       "3         corr    corr  \n",
       "4         corr    corr  \n",
       "..         ...     ...  \n",
       "953       corr    corr  \n",
       "954       corr    corr  \n",
       "955       corr    corr  \n",
       "956       corr    corr  \n",
       "957       corr    corr  \n",
       "\n",
       "[958 rows x 16 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_df\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can also be specified\n",
    "#     print(bigram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
