{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analysis list\n",
    "\n",
    "\n",
    "~~Avg CV IKI by WF group~~  \n",
    "    Difference between avg CV IKI for each WF group  \n",
    "~~Avg CV IKI by BF group~~  \n",
    "    Difference between avg CV IKI for each BF group  \n",
    "~~CV IKI for each word~~  \n",
    "    Words with lowest/highest CV IKI  \n",
    "~~CV IKI for each bigram~~  \n",
    "    ~~Bigrams with lowest/highest CV IKI~~  \n",
    "Difference between CV IKI for each word group, between individuals  \n",
    "Changes in IKI (delta IKI) across repetitions:  \n",
    "    Across all words and within word groups  \n",
    "    Delta IKI across repetitions for one IKI position within the word (rather than averaging across positions)  \n",
    "visualize average cv changes across all IKI positions for each individual \n",
    "find average IKI values for each individual  \n",
    "\n",
    "Avg. typing time per word  \n",
    "    For all words  \n",
    "    By WF  \n",
    "    By BF  \n",
    "Words typed the quickest/slowest\n",
    "Avg. time typing per bigram  \n",
    "    For all bigrams  \n",
    "    By BF  \n",
    "Bigrams typed quickest/slowest  \n",
    "\n",
    "Avg. onset delay per trial  \n",
    "    For all words  \n",
    "    By WF  \n",
    "    By BF  \n",
    "Words with quickest/slowest onset delay  \n",
    "Bigrams with quickest/slowest onset delay  \n",
    "\n",
    "Total error #  \n",
    "    by WF group  \n",
    "    by BF group  \n",
    "Words with highest error count  \n",
    "Bigrams with highest error count  \n",
    "Most common type of error  \n",
    "    By WF group  \n",
    "    By BF group  \n",
    "Avg. edit distance of errors by WF and BF  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import variation\n",
    "import glob\n",
    "import os\n",
    "import typingmod as typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## mounting to ION server\n",
    "# os.system(\"osascript -e 'mount volume \\\"smb://ion-nas.uoregon.edu\\\" \\\n",
    "#           as user name \\\"greenhouse\\\" with password \\\"password\\\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining function to organize bigrams into rows\n",
    "def bigram_byrow():\n",
    "    bigrams = []\n",
    "    for index, row in keys_intocolumns.iterrows():\n",
    "        for column in range(0, (len(keys_intocolumns.columns) - 1)):\n",
    "            if (keys_intocolumns[column][index] != None and float('nan')) and (keys_intocolumns[column + 1][index] != None and float('nan')):\n",
    "                bigram = keys_intocolumns[column][index] + keys_intocolumns[column + 1][index]\n",
    "                iki = (main_df['key_resp.rt.%(second)d' % {'second':  column + 2 }][index] - main_df['key_resp.rt.%(first)d' % { 'first': column +1 }][index])\n",
    "                bigrams.append([index, column, bigram, iki, main_df['string'][index], main_df['resp_string'][index]])\n",
    "    return(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/greenhouse/typingtask_data/subject_data/s262_01232024/psychopy_data/edited/s262_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s262_01232024/psychopy_data/edited/s262_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s261_12122023/psychopy_data/edited/s261_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s261_12122023/psychopy_data/edited/s261_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s240_11162023/psychopy_data/edited/s240_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s240_11162023/psychopy_data/edited/s240_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s217_11092023/psychopy_data/edited/s217_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s217_11092023/psychopy_data/edited/s217_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s176_10262023/psychopy_data/edited/s176_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s176_10262023/psychopy_data/edited/s176_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s20_02082024/psychopy_data/edited/s20_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s20_02082024/psychopy_data/edited/s20_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s267_02122024/psychopy_data/edited/s267_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s267_02122024/psychopy_data/edited/s267_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s263_01312024/psychopy_data/edited/s263_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s263_01312024/psychopy_data/edited/s263_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s254_11022023/psychopy_data/edited/s254_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s254_11022023/psychopy_data/edited/s254_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s09_02162024/psychopy_data/edited/s09_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s09_02162024/psychopy_data/edited/s09_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s209_09202023/psychopy_data/edited/s209_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s209_09202023/psychopy_data/edited/s209_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s276_02212024/psychopy_data/edited/s276_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s276_02212024/psychopy_data/edited/s276_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s278_02232024/psychopy_data/edited/s278_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s278_02232024/psychopy_data/edited/s278_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s279_02232024/psychopy_data/edited/s279_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s279_02232024/psychopy_data/edited/s279_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s282_02292024/psychopy_data/edited/s282_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s282_02292024/psychopy_data/edited/s282_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s283_02292024/psychopy_data/edited/s283_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s283_02292024/psychopy_data/edited/s283_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s286_03082024/psychopy_data/edited/s286_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s286_03082024/psychopy_data/edited/s286_bybigram.csv\n"
     ]
    }
   ],
   "source": [
    "## create dataframes tiral-based and bigram-based dataframes for each subject ##\n",
    "\n",
    "## importing experiment data\n",
    "test = r'/Volumes/greenhouse/typingtask_data/subject_data'\n",
    "os.chdir(test)\n",
    "folders = os.listdir()\n",
    "\n",
    "## looping through subjects\n",
    "sub_folders = list(filter(lambda x: x.startswith('s', 0, 1), folders))\n",
    "for sub in sub_folders:\n",
    "    sub_folder = r'/Volumes/greenhouse/typingtask_data/subject_data/%s/psychopy_data/' % sub\n",
    "    os.chdir(sub_folder)\n",
    "    sID = sub.split('_', 1)[0]\n",
    "    og_df = pd.read_csv(glob.glob('*.csv')[0])   \n",
    "   \n",
    "    ## deleting first 3 practice trials -- EDIT FOR ANY TRIALS YOU WANT TO IMMEDIATELY EXCLUDE\n",
    "    df = (og_df.drop(labels=[0, 1, 2], axis=0)).reset_index(drop = True) \n",
    "    \n",
    "    ## expanding nested key_resp.rt values into separate columns, making new dataframe, and turning values back into floats from strings\n",
    "    stripped_rts_1 = ((df['key_resp_1.rt'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    stripped_rts_2 = ((df['key_resp_2.rt'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    rts_intocolumns = (pd.concat([stripped_rts_1, stripped_rts_2])).reset_index(drop = True)\n",
    "    \n",
    "    ## renames rt columns to automatically match dataset\n",
    "    DF = rts_intocolumns\n",
    "    renamed_rt = DF.rename(columns = { 0:'key_resp.rt.%s' %(0+1) })\n",
    "    for n in range(0, len(DF.columns)):\n",
    "        renamed_rt = renamed_rt.rename(columns = { n:'key_resp.rt.%s' %(n+1) })\n",
    "    expanded_rts = renamed_rt.astype(float).fillna(0) ##replacing NaNs with zeroes\n",
    "\n",
    "    ## expanding nested key_resp.keys values into separate columns and making new dataframe\n",
    "    stripped_keys_1 = ((df['key_resp_1.keys'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    stripped_keys_2 = ((df['key_resp_2.keys'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    keys_intocolumns = (pd.concat([stripped_keys_1, stripped_keys_2])).reset_index(drop = True)\n",
    "    keys_intocolumns = keys_intocolumns.where(pd.notnull(keys_intocolumns), None) \n",
    "        # ^ also replaces any added NaNs with Nones\n",
    "\n",
    "    ## renames key columns to automatically match dataset\n",
    "    DF = keys_intocolumns\n",
    "    expanded_keys = DF.rename(columns = { 0:'key_resp.keys.%s' %(0+1) })\n",
    "    for n in range(0, len(DF.columns)):\n",
    "        expanded_keys = expanded_keys.rename(columns = { n:'key_resp.keys.%s' %(n+1) })\n",
    "\n",
    "    ## getting rid of apostrophes and spaces in key values\n",
    "    cols_to_change = (expanded_keys.iloc[:, 0:])\n",
    "    for col in cols_to_change:\n",
    "        expanded_keys[col] = expanded_keys[col].str.replace(\"'\", \"\")\n",
    "        expanded_keys[col] = expanded_keys[col].str.replace(\" \", \"\")\n",
    "\n",
    "    ## combining key_resp.keys into one simple string to easily represent typed responses\n",
    "    responses_1 = pd.DataFrame((df['key_resp_1.keys'].str.replace(\"[', ]\", \"\", regex=True).str.strip(\"[]\")).dropna()).rename(columns = {'key_resp_1.keys':'resp_string'})\n",
    "    responses_2 = pd.DataFrame((df['key_resp_2.keys'].str.replace(\"[', ]\", \"\", regex=True).str.strip(\"[]\")).dropna()).rename(columns = {'key_resp_2.keys':'resp_string'})\n",
    "    responses = (pd.concat([responses_1, responses_2])).reset_index(drop = True)\n",
    "\n",
    "    ## identifying bigrams in words to add to larger dataframe\n",
    "    ## defining function that separates words in to bigrams\n",
    "    def bi_byword(word):\n",
    "        bi_results = []\n",
    "        for y in range(0, (len(word)-1)):\n",
    "            bigram = word[y] + word[y+1]\n",
    "            bi_results.append(bigram)\n",
    "        return bi_results\n",
    "\n",
    "    ## defining function that separates all words into bigrams\n",
    "    def bi_allwords():\n",
    "        bigrams = []\n",
    "        for word in df['string']:\n",
    "            bigrams.append(bi_byword(word))\n",
    "        return bigrams\n",
    "\n",
    "    task_bigrams = pd.DataFrame(bi_allwords())\n",
    "    task_bigrams.columns = ['bi_1', 'bi_2', 'bi_3', 'bi_4']\n",
    "    \n",
    "    ## combining expanded rt, expanded keys, and response string values with column for strings typed each trial to create more useful dataframe\n",
    "    ## (does not have all the random timing data of other events occuring during the task)\n",
    "    main_df = pd.concat([task_bigrams, expanded_keys, expanded_rts, responses], axis = 1)\n",
    "    main_df.insert(0, 'string', df['string'], True)\n",
    "\n",
    "    ## creating column for WF type for each trial\n",
    "    main_df['WF Type'] = \"\"\n",
    "    for index, data in main_df.iterrows():\n",
    "        if main_df.loc[index, 'string'] in typ.highwf:\n",
    "            main_df.loc[index, 'WF Type'] = 'highwf'\n",
    "        if main_df.loc[index, 'string'] in typ.medwf:\n",
    "            main_df.loc[index, 'WF Type'] = 'medwf'\n",
    "        if main_df.loc[index, 'string'] in typ.lowwf:\n",
    "            main_df.loc[index, 'WF Type'] = 'lowwf'\n",
    "        if main_df.loc[index, 'string'] in typ.pseudo:\n",
    "            main_df.loc[index, 'WF Type'] = 'pseudo'\n",
    "\n",
    "    ## creating column for BF type for each trial\n",
    "    main_df['BF Type'] = \"\"\n",
    "    for index, data in main_df.iterrows():\n",
    "        if main_df.loc[index, 'string'] in typ.highbf:\n",
    "            main_df.loc[index, 'BF Type'] = 'highbf'\n",
    "        if main_df.loc[index, 'string'] in typ.medbf:\n",
    "            main_df.loc[index, 'BF Type'] = 'medbf'\n",
    "        if main_df.loc[index, 'string'] in typ.lowbf:\n",
    "            main_df.loc[index, 'BF Type'] = 'lowbf'\n",
    "\n",
    "    ## creating column for trial (useful for group analysis)\n",
    "    trial_nums = []\n",
    "    for index, data in main_df.iterrows():\n",
    "        trial_nums.append(index)\n",
    "    main_df.insert(0, 'Trial #', trial_nums)\n",
    "\n",
    "    ## creating column for subject ID (also useful for group analysis)\n",
    "    main_ID = [sID]*len(main_df)\n",
    "    main_df.insert(0, 'sID', main_ID)\n",
    "    \n",
    "    ## making csv from dataframe\n",
    "    edited_path = os.path.join(sub_folder, 'edited')\n",
    "    if os.path.exists(edited_path) == False:\n",
    "        os.mkdir(edited_path)\n",
    "    bytrial_path = os.path.join(edited_path, '%s_bytrial.csv' % sID)\n",
    "    print(bytrial_path)\n",
    "    main_df.to_csv(bytrial_path)\n",
    "    # with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can also be specified\n",
    "    #     print(main_df)\n",
    "    \n",
    "    ## make dataframe of bigram IKIs\n",
    "    bigram_df = (pd.DataFrame(bigram_byrow())).rename(columns={0: \"Trial #\", 1: \"bigram_loc\",  2:\"Bigram\", 3: \"IKI\", 4: \"string\", 5: \"resp_string\"})\n",
    "\n",
    "    ## creating column for bigram # (useful for group analysis)\n",
    "    bigram_nums = []\n",
    "    for index, data in bigram_df.iterrows():\n",
    "        bigram_nums.append(index)\n",
    "    bigram_df.insert(0, 'Bigram #', bigram_nums)\n",
    "\n",
    "    ## creating column for subject ID (also useful for group analysis)\n",
    "    bigram_ID = [sID]*len(bigram_df)\n",
    "    bigram_df.insert(0, 'sID', bigram_ID)\n",
    "\n",
    "    ## creating column for rep #\n",
    "    bigram_df['rep #'] = ''\n",
    "    for string in typ.all_strings:\n",
    "        by_string = bigram_df[bigram_df.string == string]\n",
    "        trials = list(set(by_string['Trial #']))\n",
    "        trials.sort()\n",
    "        for trial in trials:\n",
    "            by_rep = by_string[by_string['Trial #'] == trial]\n",
    "            rows = by_rep.index\n",
    "            bigram_df.loc[rows, 'rep #'] = trials.index(trial)\n",
    "\n",
    "    ## creating column for correct bigram (as opposed to the typed bigram)\n",
    "    bigram_df.insert(5, 'corr_bigram', '')\n",
    "    for index, row in bigram_df.iterrows():\n",
    "        loc = bigram_df.loc[index, 'bigram_loc']\n",
    "        loc_list = [0, 1, 2, 3]\n",
    "        if loc in loc_list:\n",
    "            corr = bi_byword(bigram_df.loc[index, 'string'])[loc]\n",
    "        else:\n",
    "            corr = ''\n",
    "        bigram_df.loc[index, 'corr_bigram'] = corr\n",
    "\n",
    "    ## creating column for bigram frequency\n",
    "    bg_freqs = pd.read_csv(r'/Users/rubi/Desktop/Github/typingexp/typing_task_analysis/bg_freqs.csv') ## EDIT TO MAKE USEFUL ON OTHER COMPUTERS\n",
    "    bg_freqs.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "    freq_dict = bg_freqs.set_index('Bigrams')['Frequency'].to_dict()\n",
    "    bigram_df['bg_freq'] = bigram_df['corr_bigram'].map(freq_dict)\n",
    "\n",
    "    ## creating column for bigram type\n",
    "    name_list = ['high', 'med', 'low', 'pseudo']\n",
    "\n",
    "    for index, bf_type in enumerate(typ.bf_types):\n",
    "        by_bf = bigram_df[bigram_df.corr_bigram.isin(bf_type)]\n",
    "        rows = by_bf.index\n",
    "        bigram_df.loc[rows, 'bf_type'] = name_list[index]\n",
    "\n",
    "    ## creating a column for mean bigram type\n",
    "    for index, avgbf_type in enumerate(typ.avgbf_types):\n",
    "        by_bf = bigram_df[bigram_df.string.isin(avgbf_type)]\n",
    "        rows = by_bf.index\n",
    "        bigram_df.loc[rows, 'meanbf_type'] = name_list[index]\n",
    "\n",
    "    ## creating a column for mean bigram type\n",
    "    for index, wf_type in enumerate(typ.wf_types):\n",
    "        by_wf = bigram_df[bigram_df.string.isin(wf_type)]\n",
    "        rows = by_wf.index\n",
    "        bigram_df.loc[rows, 'wf_type'] = name_list[index]\n",
    "\n",
    "    ## making csv from dataframe\n",
    "    bybigram_path = os.path.join(edited_path, '%s_bybigram.csv' % sID)\n",
    "    print(bybigram_path)\n",
    "    bigram_df.to_csv(bybigram_path) \n",
    "    # with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can also be specified\n",
    "    #     print(bigram_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
