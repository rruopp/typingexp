{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analysis list\n",
    "\n",
    "\n",
    "~~Avg CV IKI by WF group~~  \n",
    "    Difference between avg CV IKI for each WF group  \n",
    "~~Avg CV IKI by BF group~~  \n",
    "    Difference between avg CV IKI for each BF group  \n",
    "~~CV IKI for each word~~  \n",
    "    Words with lowest/highest CV IKI  \n",
    "~~CV IKI for each bigram~~  \n",
    "    ~~Bigrams with lowest/highest CV IKI~~  \n",
    "Difference between CV IKI for each word group, between individuals  \n",
    "Changes in IKI (delta IKI) across repetitions:  \n",
    "    Across all words and within word groups  \n",
    "    Delta IKI across repetitions for one IKI position within the word (rather than averaging across positions)  \n",
    "visualize average cv changes across all IKI positions for each individual \n",
    "find average IKI values for each individual  \n",
    "\n",
    "Avg. typing time per word  \n",
    "    For all words  \n",
    "    By WF  \n",
    "    By BF  \n",
    "Words typed the quickest/slowest\n",
    "Avg. time typing per bigram  \n",
    "    For all bigrams  \n",
    "    By BF  \n",
    "Bigrams typed quickest/slowest  \n",
    "\n",
    "Avg. onset delay per trial  \n",
    "    For all words  \n",
    "    By WF  \n",
    "    By BF  \n",
    "Words with quickest/slowest onset delay  \n",
    "Bigrams with quickest/slowest onset delay  \n",
    "\n",
    "Total error #  \n",
    "    by WF group  \n",
    "    by BF group  \n",
    "Words with highest error count  \n",
    "Bigrams with highest error count  \n",
    "Most common type of error  \n",
    "    By WF group  \n",
    "    By BF group  \n",
    "Avg. edit distance of errors by WF and BF  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing experiment data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import variation\n",
    "\n",
    "import typingmod as typ\n",
    "\n",
    "## Desktop\n",
    "og_df = pd.read_csv(r'/home/rubi/'\n",
    "                        'Desktop/Github/typingexp/typing_task_analysis/subject_data/raw/p08_marygach_08022022_test_typingtask_norepeats_2022_Aug_02_1412.csv')\n",
    "\n",
    "## Laptop\n",
    "# og_df = pd.read_csv(r'/Users/rubi/'\n",
    "#                     'Desktop/Github/typingexp/typing_task_analysis/subject_data/raw/p11_serena_09072022_test_typingtask_norepeats_2022_Sep_07_1351.csv')\n",
    "\n",
    "## IMPORTANT PRESETS\n",
    "sID = 'p08'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EDIT FOR ANY TRIALS YOU WANT TO IMMEDIATELY EXCLUDE ###\n",
    "\n",
    "## deleting first 3 practice trials\n",
    "df = (og_df.drop(labels=[0, 1, 2], axis=0)).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m         bigrams\u001b[38;5;241m.\u001b[39mappend(bi_byword(word))\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bigrams\n\u001b[0;32m---> 53\u001b[0m task_bigrams \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mbi_allwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_strings\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     54\u001b[0m task_bigrams\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbi_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbi_2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbi_3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbi_4\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mbi_allwords\u001b[0;34m(string_type)\u001b[0m\n\u001b[1;32m     48\u001b[0m bigrams \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 50\u001b[0m     bigrams\u001b[38;5;241m.\u001b[39mappend(\u001b[43mbi_byword\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bigrams\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mbi_byword\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbi_byword\u001b[39m(word):\n\u001b[1;32m     40\u001b[0m     bi_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, (\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m     42\u001b[0m         bigram \u001b[38;5;241m=\u001b[39m word[y] \u001b[38;5;241m+\u001b[39m word[y\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     43\u001b[0m         bi_results\u001b[38;5;241m.\u001b[39mappend(bigram)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "## expanding nested key_resp.rt values into separate columns, making new dataframe, and turning values back into floats from strings\n",
    "stripped_rts_1 = ((df['key_resp_1.rt'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "stripped_rts_2 = ((df['key_resp_2.rt'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "rts_intocolumns = (pd.concat([stripped_rts_1, stripped_rts_2])).reset_index(drop = True)\n",
    "\n",
    "## renames rt columns to automatically match dataset\n",
    "DF = rts_intocolumns\n",
    "renamed_rt = DF.rename(columns = { 0:'key_resp.rt.%s' %(0+1) })\n",
    "for n in range(0, len(DF.columns)):\n",
    "    renamed_rt = renamed_rt.rename(columns = { n:'key_resp.rt.%s' %(n+1) })\n",
    "expanded_rts = renamed_rt.astype(float).fillna(0) ##replacing NaNs with zeroes\n",
    "\n",
    "## expanding nested key_resp.keys values into separate columns and making new dataframe\n",
    "stripped_keys_1 = ((df['key_resp_1.keys'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "stripped_keys_2 = ((df['key_resp_2.keys'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "keys_intocolumns = (pd.concat([stripped_keys_1, stripped_keys_2])).reset_index(drop = True)\n",
    "keys_intocolumns = keys_intocolumns.where(pd.notnull(keys_intocolumns), None) \n",
    "    # ^ also replaces any added NaNs with Nones\n",
    "\n",
    "## renames key columns to automatically match dataset\n",
    "DF = keys_intocolumns\n",
    "expanded_keys = DF.rename(columns = { 0:'key_resp.keys.%s' %(0+1) })\n",
    "for n in range(0, len(DF.columns)):\n",
    "    expanded_keys = expanded_keys.rename(columns = { n:'key_resp.keys.%s' %(n+1) })\n",
    "    \n",
    "## getting rid of apostrophes and spaces in key values\n",
    "cols_to_change = (expanded_keys.iloc[:, 0:])\n",
    "for col in cols_to_change:\n",
    "    expanded_keys[col] = expanded_keys[col].str.replace(\"'\", \"\")\n",
    "    expanded_keys[col] = expanded_keys[col].str.replace(\" \", \"\")\n",
    "    \n",
    "## combining key_resp.keys into one simple string to easily represent typed responses\n",
    "responses_1 = pd.DataFrame((df['key_resp_1.keys'].str.replace(\"[', ]\", \"\", regex=True).str.strip(\"[]\")).dropna()).rename(columns = {'key_resp_1.keys':'resp_string'})\n",
    "responses_2 = pd.DataFrame((df['key_resp_2.keys'].str.replace(\"[', ]\", \"\", regex=True).str.strip(\"[]\")).dropna()).rename(columns = {'key_resp_2.keys':'resp_string'})\n",
    "responses = (pd.concat([responses_1, responses_2])).reset_index(drop = True)\n",
    "\n",
    "## identifying bigrams in words to add to larger dataframe\n",
    "## defining function that separates words in to bigrams\n",
    "def bi_byword(word):\n",
    "    bi_results = []\n",
    "    for y in range(0, (len(word)-1)):\n",
    "        bigram = word[y] + word[y+1]\n",
    "        bi_results.append(bigram)\n",
    "    return bi_results\n",
    "\n",
    "## defining function that separates all words into bigrams\n",
    "def bi_allwords(string_type):\n",
    "    bigrams = []\n",
    "    for word in df['string']:\n",
    "        bigrams.append(bi_byword(word))\n",
    "    return bigrams\n",
    "\n",
    "task_bigrams = pd.DataFrame(bi_allwords(typ.all_strings))\n",
    "task_bigrams.columns = ['bi_1', 'bi_2', 'bi_3', 'bi_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combining expanded rt, expanded keys, and response string values with column for strings typed each trial to create more useful dataframe\n",
    "    ## (does not have all the random timing data of other events occuring during the task)\n",
    "main_df = pd.concat([task_bigrams, expanded_keys, expanded_rts, responses], axis = 1)\n",
    "main_df.insert(0, 'string', df['string'], True)\n",
    "\n",
    "## creating column for WF type for each trial\n",
    "main_df['WF Type'] = \"\"\n",
    "for index, data in main_df.iterrows():\n",
    "    if main_df.loc[index, 'string'] in typ.highwf:\n",
    "        main_df.loc[index, 'WF Type'] = 'highwf'\n",
    "    if main_df.loc[index, 'string'] in typ.medwf:\n",
    "        main_df.loc[index, 'WF Type'] = 'medwf'\n",
    "    if main_df.loc[index, 'string'] in typ.lowwf:\n",
    "        main_df.loc[index, 'WF Type'] = 'lowwf'\n",
    "    if main_df.loc[index, 'string'] in typ.pseudo:\n",
    "        main_df.loc[index, 'WF Type'] = 'pseudo'\n",
    "\n",
    "## creating column for BF type for each trial\n",
    "main_df['BF Type'] = \"\"\n",
    "for index, data in main_df.iterrows():\n",
    "    if main_df.loc[index, 'string'] in typ.highbf:\n",
    "        main_df.loc[index, 'BF Type'] = 'highbf'\n",
    "    if main_df.loc[index, 'string'] in typ.medbf:\n",
    "        main_df.loc[index, 'BF Type'] = 'medbf'\n",
    "    if main_df.loc[index, 'string'] in typ.lowbf:\n",
    "        main_df.loc[index, 'BF Type'] = 'lowbf'\n",
    "        \n",
    "## creating column for trial (useful for group analysis)\n",
    "trial_nums = []\n",
    "for index, data in main_df.iterrows():\n",
    "    trial_nums.append(index)\n",
    "main_df.insert(0, 'Trial #', trial_nums)\n",
    "\n",
    "## creating column for subject ID (also useful for group analysis)\n",
    "main_ID = [sID]*len(main_df)\n",
    "main_df.insert(0, 'sID', main_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## making csv from dataframe\n",
    "main_path = 'subject_data/main_dfs/s%s_maindf.csv' % (sID)\n",
    "main_df.to_csv(main_path)\n",
    "main_df\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can also be specified\n",
    "#     print(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## make dataframe of bigram IKIs\n",
    "## bigram / trial # / string / IKI / bf type / resp_string / string type \n",
    "def bigram_byrow():\n",
    "    bigrams = []\n",
    "    for index, row in keys_intocolumns.iterrows():\n",
    "        for column in range(0, (len(keys_intocolumns.columns) - 1)):\n",
    "            if (keys_intocolumns[column][index] != None and float('nan')) and (keys_intocolumns[column + 1][index] != None and float('nan')):\n",
    "                bigram = keys_intocolumns[column][index] + keys_intocolumns[column + 1][index]\n",
    "                iki = (main_df['key_resp.rt.%(second)d' % {'second':  column + 2 }][index] - main_df['key_resp.rt.%(first)d' % { 'first': column +1 }][index])\n",
    "                bigrams.append([index, bigram, iki, main_df['string'][index], main_df['resp_string'][index]])\n",
    "    return(bigrams)\n",
    "    \n",
    "bigram_df = (pd.DataFrame(bigram_byrow())).rename(columns={0: \"Trial #\", 1: \"Bigram\", 2: \"IKI\", 3: \"string\", 4: \"resp_string\"})\n",
    "\n",
    "## creating column for bigram # (useful for group analysis)\n",
    "bigram_nums = []\n",
    "for index, data in bigram_df.iterrows():\n",
    "    bigram_nums.append(index)\n",
    "bigram_df.insert(0, 'Bigram #', bigram_nums)\n",
    "\n",
    "## creating column for subject ID (also useful for group analysis)\n",
    "bigram_ID = [sID]*len(bigram_df)\n",
    "bigram_df.insert(0, 'sID', bigram_ID)\n",
    "\n",
    "## creating column for rep #\n",
    "bigram_df['rep #'] = ''\n",
    "for string in typ.all_strings:\n",
    "    by_string = bigram_df[bigram_df.string == string]\n",
    "    trials = list(set(by_string['Trial #']))\n",
    "    trials.sort()\n",
    "    for trial in trials:\n",
    "        by_rep = by_string[by_string['Trial #'] == trial]\n",
    "        rows = by_rep.index\n",
    "        bigram_df.loc[rows, 'rep #'] = trials.index(trial)\n",
    "\n",
    "## making csv from dataframe\n",
    "bigram_path = 'subject_data/bigram_dfs/%s_bigramdf.csv' % (sID)\n",
    "bigram_df.to_csv(bigram_path)\n",
    "bigram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## making filtered dataframes\n",
    "all_rts = typ.rt_columns(main_df)\n",
    "\n",
    "corr_df = typ.correct_filter(main_df) ## only correct trials\n",
    "incorr_df = typ.incorrect_filter(main_df) ## only incorrect trials\n",
    "\n",
    "corr_rts = typ.rt_columns(corr_df) ## only correct rts\n",
    "incorr_rts = typ.rt_columns(incorr_df) ## only incorrect rts\n",
    "\n",
    "## making filtered dataframes by word frequency\n",
    "highwf_df = main_df[main_df['string'].isin(typ.highwf)]\n",
    "medwf_df = main_df[main_df['string'].isin(typ.medwf)]\n",
    "lowwf_df = main_df[main_df\n",
    "                   ['string'].isin(typ.lowwf)]\n",
    "pseudo_df = main_df[main_df['string'].isin(typ.pseudo)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## making dataframe of only rts from all trials\n",
    "# all_rts = expanded_rts\n",
    "# ## inserting column containing the string typed during each trial, if it hasn't already been inserted\n",
    "# if ((all_rts.iloc[:, 0]).dtype) != object:\n",
    "#     all_rts.insert(0, 'string', df['string'], True)\n",
    "\n",
    "## making dataframes with rts by word frequency type\n",
    "highwf_rts = (typ.rt_columns(highwf_df)).reset_index(drop=True)\n",
    "medwf_rts = (typ.rt_columns(medwf_df)).reset_index(drop=True)\n",
    "lowwf_rts = (typ.rt_columns(lowwf_df)).reset_index(drop=True)\n",
    "pseudo_rts = (typ.rt_columns(pseudo_df)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IKI Variables by Word and Word Type (WF and BF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WIP -- need to get strings out of list to get mean\n",
    "\n",
    "# defining function to find average IKI across all string positions for each trial\n",
    "# def avg_iki(str_type, DF):\n",
    "#     all_ikis = iki(DF)\n",
    "#     sort_byword = []\n",
    "#     for index, data in all_ikis.iterrows():\n",
    "#             if all_ikis['string'][index] in str_type:\n",
    "#                 sort_byword.append(data)\n",
    "#     iki_byword = pd.DataFrame(sort_byword)\n",
    "#     iki_byword.reset_index(drop=True, inplace=True)\n",
    "#     iki_list = iki_byword.values.tolist()\n",
    "#     iki_avgs = sum(iki_list)/len(iki_list)\n",
    "#     return iki_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## creating dataframe from 'cv_byword' function that is easy to plot\n",
    "cv_word = 'druze' # change this for which ever word you want to plot\n",
    "cv_byword_df = (pd.DataFrame(typ.cv_byword(cv_word, corr_rts)).drop(0)).reset_index(drop = True)\n",
    "\n",
    "## plotting digraph latency variations by word (does not include error trials)\n",
    "plt.figure(figsize=(10, 7))\n",
    "x = [0, 1, 2, 3]\n",
    "highwf_plot = plt.plot(cv_byword_df, color='red', label=cv_word)\n",
    "# medwf_plot = plt.plot(cv_byword_df, color='red', label='There')\n",
    "\n",
    "plt.xticks(np.arange(min(x), max(x)+1, 1.0))\n",
    "plt.xlabel(\"Latency position in string\")\n",
    "plt.ylabel(\"Average CV\")\n",
    "plt.ylim([0, 0.6])\n",
    "plt.title(\"CV of Interkey Interval by Word\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting digraph latency variations by WF type (includes error trials)\n",
    "## NOTE: run twice for tick marks / label sizes to update -- dunno why\n",
    "plt.figure(figsize=(10, 7))\n",
    "x = [0, 1, 2, 3]\n",
    "highwf_plot = plt.plot(typ.avg_cv(typ.highwf, all_rts), color='red', label='High WF')\n",
    "medwf_plot = plt.plot(typ.avg_cv(typ.medwf, all_rts), color='blue', label='Medium WF')\n",
    "lowwf_plot = plt.plot(typ.avg_cv(typ.lowwf, all_rts), color='green', label='Low WF')\n",
    "pseudo_plot = plt.plot(typ.avg_cv(typ.pseudo, all_rts), color='orange', label='Pseudo')\n",
    "\n",
    "## Set general font size\n",
    "plt.rcParams['font.size'] = '25'\n",
    "\n",
    "## Set tick font size\n",
    "# for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "#     label.set_fontsize(25)\n",
    "\n",
    "plt.xticks(np.arange(min(x), max(x)+1, 1.0))\n",
    "plt.xlabel(\"Interval position in string\")\n",
    "plt.ylabel(\"Average CV\")\n",
    "plt.ylim([0, 0.6])\n",
    "plt.title(\"Average CV of Interkey Interval by Word Frequency (w/ error)\", pad=30)\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "# plt.savefig('ian_iki.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting digraph latency variations by WF type (does not include error trials)\n",
    "plt.figure(figsize=(10, 7))\n",
    "x = [0, 1, 2, 3]\n",
    "highwf_plot = plt.plot(typ.avg_cv(typ.highwf, corr_rts), color='red', label='High WF')\n",
    "medwf_plot = plt.plot(typ.avg_cv(typ.medwf, corr_rts), color='blue', label='Medium WF')\n",
    "lowwf_plot = plt.plot(typ.avg_cv(typ.lowwf, corr_rts), color='green', label='Low WF')\n",
    "pseudo_plot = plt.plot(typ.avg_cv(typ.pseudo, corr_rts), color='orange', label='Pseudo')\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(min(x), max(x)+1, 1.0))\n",
    "plt.xlabel(\"Latency position in string\")\n",
    "plt.ylabel(\"Average CV\")\n",
    "plt.ylim([0, 0.6])\n",
    "plt.title(\"Average CV of Interkey Interval by WF Type (w/out error)\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## plotting digraph latency variations by BF type (includes error trials)\n",
    "plt.figure(figsize=(10, 7))\n",
    "x = [0, 1, 2, 3]\n",
    "highbf_plot = plt.plot(typ.avg_cv(typ.highbf, all_rts), color='red', label='High bigram freq.')\n",
    "medbf_plot = plt.plot(typ.avg_cv(typ.medbf, all_rts), color='blue', label='Medium bigram freq.')\n",
    "lowbf_plot = plt.plot(typ.avg_cv(typ.lowbf, all_rts), color='green', label='Low bigram freq.')\n",
    "\n",
    "plt.xticks(np.arange(min(x), max(x)+1, 1.0))\n",
    "plt.xlabel(\"Latency position in string\")\n",
    "plt.ylabel(\"Average CV\")\n",
    "plt.ylim([0, 0.6])\n",
    "plt.title(\"Average CV of Interkey Interval by BF Type (w/ error)\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting digraph latency variations by BF type (does not include error trials)\n",
    "plt.figure(figsize=(10, 7))\n",
    "x = [0, 1, 2, 3]\n",
    "highbf_plot = plt.plot(typ.avg_cv(typ.highbf, corr_rts), color='red', label='High bigram freq.')\n",
    "medbf_plot = plt.plot(typ.avg_cv(typ.medbf, corr_rts), color='blue', label='Medium bigram freq.')\n",
    "lowbf_plot = plt.plot(typ.avg_cv(typ.lowbf, corr_rts), color='green', label='Low bigram freq.')\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(min(x), max(x)+1, 1.0))\n",
    "plt.xlabel(\"Latency position in string\")\n",
    "plt.ylabel(\"Average CV\")\n",
    "plt.ylim([0, 0.6])\n",
    "plt.title(\"Average CV of Interkey Interval by BF Type (w/out error)\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IKI Variables by Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## making dataframe of bigram cv ikis and organizing in descending order\n",
    "bigram_cvs = (pd.DataFrame(typ.cv_allbgs(bigram_df))).rename(columns = {0 : \"Bigram\", 1 : \"CV IKI\"}).sort_values(by = ['CV IKI'], ignore_index = True)\n",
    "bigram_cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating dataframes with bigram cv iki maxima and minima\n",
    "bg_maxima = bigram_cvs.loc[0:4]\n",
    "bg_minima = bigram_cvs.loc[65:69]\n",
    "bg_maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Onset Delay Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting onset delay by wf type as scatter plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "highwf_scatter = plt.scatter(typ.onset_delay(highwf_df), [0] * len(typ.onset_delay(highwf_df)), c=['red'] * len(typ.onset_delay(highwf_df)), label='Most familiar')\n",
    "medwf_scatter = plt.scatter((typ.onset_delay(medwf_df)), [1] * len(typ.onset_delay(medwf_df)), c=['blue'] * len(typ.onset_delay(medwf_df)), label='Semi familiar')\n",
    "lowwf_scatter = plt.scatter((typ.onset_delay(lowwf_df)), [2] * len(typ.onset_delay(lowwf_df)), c=['green'] * len(typ.onset_delay(lowwf_df)), label='Unfamiliar')\n",
    "pseudo_scatter = plt.scatter((typ.onset_delay(pseudo_df)), [3] * len(typ.onset_delay(pseudo_df)), c=['orange'] * len(typ.onset_delay(pseudo_df)), label='Non familiar')\n",
    "\n",
    "plt.xlabel(\"Delay (s)\")\n",
    "plt.ylabel(\"String Type\")\n",
    "plt.title(\"Onset Delay by String Type\")\n",
    "plt.legend(loc = \"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting onset delay as box and whisker\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "bp = ax.boxplot([ typ.onset_delay(highwf_df), typ.onset_delay(medwf_df), typ.onset_delay(lowwf_df), typ.onset_delay(pseudo_df)])\n",
    "\n",
    "colors = ['#0000FF', '#00FF00','#FFFF00', '#FF00FF']\n",
    "\n",
    "ax.set_xticklabels(['high wf', 'med wf',\n",
    "                    'low wf', 'pseudo'])\n",
    "for median in bp['medians']:\n",
    "    \n",
    "    median.set(linewidth = 3)\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Delay (s)\")\n",
    "plt.title(\"Onset Delay by String Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typing Time Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting scatter plot of typing time with legend\n",
    "plt.figure(figsize=(10, 7))\n",
    "highwf_scatter = plt.scatter(typ.time_typing(highwf_rts), [0] * len(typ.time_typing(highwf_rts)), c=['red'] * len(typ.time_typing(highwf_rts)), label='High WF')\n",
    "medwf_scatter = plt.scatter(typ.time_typing(medwf_rts), [1] * len(typ.time_typing(medwf_rts)), c=['blue'] * len(typ.time_typing(medwf_rts)), label='Med WF')\n",
    "lowwf_scatter = plt.scatter(typ.time_typing(lowwf_rts), [2] * len(typ.time_typing(lowwf_rts)), c=['green'] * len(typ.time_typing(lowwf_rts)), label='Low WF')\n",
    "pseudo_scatter = plt.scatter(typ.time_typing(pseudo_rts), [3] * len(typ.time_typing(pseudo_rts)), c=['orange'] * len(typ.time_typing(pseudo_rts)), label='Pseudo')\n",
    "\n",
    "plt.xlabel(\"Typing Time (s)\")\n",
    "plt.ylabel(\"String Type\")\n",
    "plt.title(\"Typing Time by String Type\")\n",
    "plt.legend(loc= \"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting typing time as box and whisker\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "bp = ax.boxplot([typ.time_typing(highwf_rts), typ.time_typing(medwf_rts), typ.time_typing(lowwf_rts), typ.time_typing(pseudo_rts)])\n",
    "\n",
    "colors = ['#0000FF', '#00FF00','#FFFF00', '#FF00FF']\n",
    "\n",
    "ax.set_xticklabels(['hig hwf', 'med wf',\n",
    "                    'low wf', 'pseudo'])\n",
    "for median in bp['medians']:\n",
    "    median.set(linewidth = 3)\n",
    "    \n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Typing Time (s)\")\n",
    "plt.title(\"Typing Time by String Type\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta IKI Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting average delta IKI by WF type (only for correct trials)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "x = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "highwf_plot = plt.plot((typ.avg_diki_byword('think', corr_rts)), color='red', label='\"Right\"')\n",
    "medwf_plot = plt.plot((typ.avg_diki_byword('there', corr_rts)), color='blue', label='\"There\"')\n",
    "lowwf_plot = plt.plot((typ.avg_diki_byword('about', corr_rts)), color='green', label='\"About\"')\n",
    "pseudo_plot = plt.plot((typ.avg_diki_byword('would', corr_rts)), color='orange', label='\"Where\"')\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(min(x), max(x)+1, 1.0))\n",
    "plt.xlabel(\"Inter-repetition position\", labelpad=15)\n",
    "plt.ylabel(\"Average interkey interval difference\", labelpad=15)\n",
    "# plt.ylim([0.1, 0.6])\n",
    "plt.title(\"Average Interkey Interval Difference by String\", pad=40)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
