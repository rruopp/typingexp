{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import variation\n",
    "import glob\n",
    "import os\n",
    "import typingmod as typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## mounting to ION server\n",
    "# os.system(\"osascript -e 'mount volume \\\"smb://ion-nas.uoregon.edu\\\" \\\n",
    "#           as user name \\\"greenhouse\\\" with password \\\"password\\\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining function to organize bigrams into rows\n",
    "def bigram_byrow():\n",
    "    bigrams = []\n",
    "    for index, row in keys_intocolumns.iterrows():\n",
    "        for column in range(0, (len(keys_intocolumns.columns) - 1)):\n",
    "            if (keys_intocolumns[column][index] != None and float('nan')) and (keys_intocolumns[column + 1][index] != None and float('nan')):\n",
    "                bigram = (keys_intocolumns[column][index] + keys_intocolumns[column + 1][index])\n",
    "                bigram = (bigram.replace(\"'\", \"\")).replace(\" \", \"\")\n",
    "                iki = (main_df['key_resp.rt.%(second)d' % {'second':  column + 2 }][index] - main_df['key_resp.rt.%(first)d' % { 'first': column +1 }][index])\n",
    "                bigrams.append([index, column, bigram, iki, main_df['string'][index], main_df['resp_string'][index]])\n",
    "    return(bigrams)\n",
    "\n",
    "## defining function that separates words in to bigrams\n",
    "def bi_byword(word):\n",
    "    bi_results = []\n",
    "    for y in range(0, (len(word)-1)):\n",
    "        bigram = word[y] + word[y+1]\n",
    "        bi_results.append(bigram)\n",
    "    return bi_results\n",
    "\n",
    "## defining function that separates all words into bigrams\n",
    "def bi_allwords():\n",
    "    bigrams = []\n",
    "    for word in df['string']:\n",
    "        bigrams.append(bi_byword(word))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/greenhouse/typingtask_data/subject_data/s262_01232024/psychopy_data/edited/s262_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s262_01232024/psychopy_data/edited/s262_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s261_12122023/psychopy_data/edited/s261_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s261_12122023/psychopy_data/edited/s261_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s240_11162023/psychopy_data/edited/s240_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s240_11162023/psychopy_data/edited/s240_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s217_11092023/psychopy_data/edited/s217_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s217_11092023/psychopy_data/edited/s217_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s176_10262023/psychopy_data/edited/s176_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s176_10262023/psychopy_data/edited/s176_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s20_02082024/psychopy_data/edited/s20_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s20_02082024/psychopy_data/edited/s20_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s267_02122024/psychopy_data/edited/s267_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s267_02122024/psychopy_data/edited/s267_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s263_01312024/psychopy_data/edited/s263_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s263_01312024/psychopy_data/edited/s263_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s254_11022023/psychopy_data/edited/s254_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s254_11022023/psychopy_data/edited/s254_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s09_02162024/psychopy_data/edited/s09_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s09_02162024/psychopy_data/edited/s09_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s209_09202023/psychopy_data/edited/s209_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s209_09202023/psychopy_data/edited/s209_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s276_02212024/psychopy_data/edited/s276_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s276_02212024/psychopy_data/edited/s276_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s278_02232024/psychopy_data/edited/s278_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s278_02232024/psychopy_data/edited/s278_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s279_02232024/psychopy_data/edited/s279_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s279_02232024/psychopy_data/edited/s279_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s282_02292024/psychopy_data/edited/s282_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s282_02292024/psychopy_data/edited/s282_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s283_02292024/psychopy_data/edited/s283_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s283_02292024/psychopy_data/edited/s283_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s286_03082024/psychopy_data/edited/s286_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s286_03082024/psychopy_data/edited/s286_bybigram.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s302_05282024/psychopy_data/edited/s302_bytrial.csv\n",
      "/Volumes/greenhouse/typingtask_data/subject_data/s302_05282024/psychopy_data/edited/s302_bybigram.csv\n"
     ]
    }
   ],
   "source": [
    "## create dataframes tiral-based and bigram-based dataframes for each subject ##\n",
    "\n",
    "## importing experiment data\n",
    "server = r'/Volumes/greenhouse/typingtask_data/subject_data'\n",
    "server_noturbo = r'/Volumes/greenhouse/typingtask_data/subject_data/not_used/no_turbotyping/'\n",
    "os.chdir(server)\n",
    "folders = os.listdir()\n",
    "\n",
    "# looping through subjects\n",
    "sub_folders = list(filter(lambda x: x.startswith('s', 0, 1), folders))\n",
    "for sub in sub_folders:\n",
    "    sub_folder = r'/Volumes/greenhouse/typingtask_data/subject_data/%s/psychopy_data/' % sub\n",
    "    os.chdir(sub_folder)\n",
    "    sID = sub.split('_', 1)[0]\n",
    "    og_df = pd.read_csv(glob.glob('*.csv')[0])   \n",
    "\n",
    "## filters through subjects without turbotyping data\n",
    "# sub_folders = list(filter(lambda x: x.startswith('s', 0, 1), folders))\n",
    "# for sub in sub_folders:\n",
    "#     sub_folder = server_noturbo + r'%s/psychopy_data/' % sub\n",
    "#     os.chdir(sub_folder)\n",
    "#     sID = sub.split('_', 1)[0]\n",
    "#     og_df = pd.read_csv(glob.glob('*.csv')[0])  \n",
    "   \n",
    "    ## deleting first 3 practice trials -- EDIT FOR ANY TRIALS YOU WANT TO IMMEDIATELY EXCLUDE\n",
    "    df = (og_df.drop(labels=[0, 1, 2], axis=0)).reset_index(drop = True) \n",
    "    \n",
    "    ## expanding nested key_resp.rt values into separate columns, making new dataframe, and turning values back into floats from strings\n",
    "    stripped_rts_1 = ((df['key_resp_1.rt'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    stripped_rts_2 = ((df['key_resp_2.rt'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    rts_intocolumns = (pd.concat([stripped_rts_1, stripped_rts_2])).reset_index(drop = True)\n",
    "    \n",
    "    ## renames rt columns to automatically match dataset\n",
    "    DF = rts_intocolumns\n",
    "    renamed_rt = DF.rename(columns = { 0:'key_resp.rt.%s' %(0+1) })\n",
    "    for n in range(0, len(DF.columns)):\n",
    "        renamed_rt = renamed_rt.rename(columns = { n:'key_resp.rt.%s' %(n+1) })\n",
    "    expanded_rts = renamed_rt.astype(float).fillna(0) ##replacing NaNs with zeroes\n",
    "\n",
    "    ## expanding nested key_resp.keys values into separate columns and making new dataframe\n",
    "    stripped_keys_1 = ((df['key_resp_1.keys'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    stripped_keys_2 = ((df['key_resp_2.keys'].str.strip('[,]')).dropna()).str.split(',', expand = True)\n",
    "    keys_intocolumns = (pd.concat([stripped_keys_1, stripped_keys_2])).reset_index(drop = True)\n",
    "    keys_intocolumns = keys_intocolumns.where(pd.notnull(keys_intocolumns), None) \n",
    "        # ^ also replaces any added NaNs with Nones\n",
    "\n",
    "    ## renames key columns to automatically match dataset\n",
    "    DF = keys_intocolumns\n",
    "    expanded_keys = DF.rename(columns = { 0:'key_resp.keys.%s' %(0+1) })\n",
    "    for n in range(0, len(DF.columns)):\n",
    "        expanded_keys = expanded_keys.rename(columns = { n:'key_resp.keys.%s' %(n+1) })\n",
    "\n",
    "    ## getting rid of apostrophes and spaces in key values\n",
    "    cols_to_change = (expanded_keys.iloc[:, 0:])\n",
    "    for col in cols_to_change:\n",
    "        expanded_keys[col] = expanded_keys[col].str.replace(\"'\", \"\")\n",
    "        expanded_keys[col] = expanded_keys[col].str.replace(\" \", \"\")\n",
    "\n",
    "    ## combining key_resp.keys into one simple string to easily represent typed responses\n",
    "    responses_1 = pd.DataFrame((df['key_resp_1.keys'].str.replace(\"[', ]\", \"\", regex=True).str.strip(\"[]\")).dropna()).rename(columns = {'key_resp_1.keys':'resp_string'})\n",
    "    responses_2 = pd.DataFrame((df['key_resp_2.keys'].str.replace(\"[', ]\", \"\", regex=True).str.strip(\"[]\")).dropna()).rename(columns = {'key_resp_2.keys':'resp_string'})\n",
    "    responses = (pd.concat([responses_1, responses_2])).reset_index(drop = True)\n",
    "\n",
    "    ## identifying bigrams in words to add to larger dataframe\n",
    "    task_bigrams = pd.DataFrame(bi_allwords())\n",
    "    task_bigrams.columns = ['bi_1', 'bi_2', 'bi_3', 'bi_4']\n",
    "    \n",
    "    ## combining expanded rt, expanded keys, and response string values with column for strings typed each trial to create more useful dataframe\n",
    "    ## (does not have all the random timing data of other events occuring during the task)\n",
    "    main_df = pd.concat([responses, task_bigrams, expanded_keys, expanded_rts], axis = 1)\n",
    "    main_df.insert(0, 'string', df['string'], True)\n",
    "\n",
    "    ## creating column for WF type for each trial\n",
    "    main_df['wf_type'] = \"\"\n",
    "    for index, data in main_df.iterrows():\n",
    "        if main_df.loc[index, 'string'] in typ.highwf:\n",
    "            main_df.loc[index, 'wf_type'] = 'highwf'\n",
    "        if main_df.loc[index, 'string'] in typ.medwf:\n",
    "            main_df.loc[index, 'wf_type'] = 'medwf'\n",
    "        if main_df.loc[index, 'string'] in typ.lowwf:\n",
    "            main_df.loc[index, 'wf_type'] = 'lowwf'\n",
    "        if main_df.loc[index, 'string'] in typ.pseudo:\n",
    "            main_df.loc[index, 'wf_type'] = 'pseudo'\n",
    "\n",
    "    ## creating column for BF type for each trial\n",
    "    main_df['meanbf_type'] = \"\"\n",
    "    for index, data in main_df.iterrows():\n",
    "        if main_df.loc[index, 'string'] in typ.avg_highbf:\n",
    "            main_df.loc[index, 'meanbf_type'] = 'highbf'\n",
    "        if main_df.loc[index, 'string'] in typ.avg_medbf:\n",
    "            main_df.loc[index, 'meanbf_type'] = 'medbf'\n",
    "        if main_df.loc[index, 'string'] in typ.avg_lowbf:\n",
    "            main_df.loc[index, 'meanbf_type'] = 'lowbf'\n",
    "\n",
    "    ## creating column for trial (useful for group analysis)\n",
    "    trial_nums = []\n",
    "    for index, data in main_df.iterrows():\n",
    "        trial_nums.append(index)\n",
    "    main_df.insert(0, 'trial_num', trial_nums)\n",
    "\n",
    "    ## creating column for subject ID (also useful for group analysis)\n",
    "    main_ID = [sID]*len(main_df)\n",
    "    main_df.insert(0, 'sID', main_ID)\n",
    "\n",
    "    ## creating columns for word repetition number\n",
    "    main_df.insert(2, 'rep_num', '')\n",
    "    main_df['rep_num'] = main_df.groupby(['sID', 'string']).cumcount()\n",
    "    \n",
    "    ## making csv from dataframe\n",
    "    edited_path = os.path.join(sub_folder, 'edited')\n",
    "    if os.path.exists(edited_path) == False:\n",
    "        os.mkdir(edited_path)\n",
    "    bytrial_path = os.path.join(edited_path, '%s_bytrial.csv' % sID)\n",
    "    print(bytrial_path)\n",
    "    main_df.to_csv(bytrial_path)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ## BIGRAM DATAFRAME ##\n",
    "    bigram_df = (pd.DataFrame(bigram_byrow())).rename(columns={0: \"trial_num\", 1: \"bigram_loc\",  2:\"resp_bigram\", 3: \"IKI\", 4: \"string\", 5: \"resp_string\"})\n",
    "\n",
    "    ## creating column for bigram # (useful for group analysis)\n",
    "    bigram_nums = []\n",
    "    for index, data in bigram_df.iterrows():\n",
    "        bigram_nums.append(index)\n",
    "    bigram_df.insert(0, 'bigram_num', bigram_nums)\n",
    "\n",
    "    ## creating column for subject ID (also useful for group analysis)\n",
    "    bigram_ID = [sID]*len(bigram_df)\n",
    "    bigram_df.insert(0, 'sID', bigram_ID)\n",
    "\n",
    "    ## creating column for correct bigram (as opposed to the typed bigram)\n",
    "    bigram_df.insert(4, 'bigram', '')\n",
    "    for index, row in bigram_df.iterrows():\n",
    "        loc = bigram_df.loc[index, 'bigram_loc']\n",
    "        loc_list = [0, 1, 2, 3]\n",
    "        if loc in loc_list:\n",
    "            corr = bi_byword(bigram_df.loc[index, 'string'])[loc]\n",
    "        else:\n",
    "            corr = ''\n",
    "        bigram_df.loc[index, 'bigram'] = corr\n",
    "\n",
    "    ## creating column for rep #\n",
    "    bigram_df.insert(3, 'rep_num', '')\n",
    "    bigram_df['rep_num'] = bigram_df.groupby(['sID', 'string', 'bigram']).cumcount()\n",
    "    \n",
    "    ## creating column for bigram frequency\n",
    "    bg_freqs = pd.read_csv(r'/Users/rubi/Desktop/Github/typingexp/typing_task_analysis/bg_freqs.csv') ## EDIT TO MAKE USEFUL ON OTHER COMPUTERS\n",
    "    bg_freqs.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "    freq_dict = bg_freqs.set_index('Bigrams')['Frequency'].to_dict()\n",
    "    bigram_df['bg_freq'] = bigram_df['bigram'].map(freq_dict)\n",
    "\n",
    "    ## creating column for bigram type\n",
    "    name_list = ['high', 'med', 'low', 'pseudo']\n",
    "\n",
    "    for index, bf_type in enumerate(typ.bf_types):\n",
    "        by_bf = bigram_df[bigram_df.bigram.isin(bf_type)]\n",
    "        rows = by_bf.index\n",
    "        bigram_df.loc[rows, 'bf_type'] = name_list[index]\n",
    "\n",
    "    ## creating a column for mean bigram type\n",
    "    for index, avgbf_type in enumerate(typ.avgbf_types):\n",
    "        by_bf = bigram_df[bigram_df.string.isin(avgbf_type)]\n",
    "        rows = by_bf.index\n",
    "        bigram_df.loc[rows, 'meanbf_type'] = name_list[index]\n",
    "\n",
    "    ## creating a column for mean bigram type\n",
    "    for index, wf_type in enumerate(typ.wf_types):\n",
    "        by_wf = bigram_df[bigram_df.string.isin(wf_type)]\n",
    "        rows = by_wf.index\n",
    "        bigram_df.loc[rows, 'wf_type'] = name_list[index]\n",
    "\n",
    "    ## making csv from dataframe\n",
    "    bybigram_path = os.path.join(edited_path, '%s_bybigram.csv' % sID)\n",
    "    print(bybigram_path)\n",
    "    bigram_df.to_csv(bybigram_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sID</th>\n",
       "      <th>trial_num</th>\n",
       "      <th>rep_num</th>\n",
       "      <th>string</th>\n",
       "      <th>resp_string</th>\n",
       "      <th>bi_1</th>\n",
       "      <th>bi_2</th>\n",
       "      <th>bi_3</th>\n",
       "      <th>bi_4</th>\n",
       "      <th>key_resp.keys.1</th>\n",
       "      <th>...</th>\n",
       "      <th>key_resp.keys.5</th>\n",
       "      <th>key_resp.keys.6</th>\n",
       "      <th>key_resp.rt.1</th>\n",
       "      <th>key_resp.rt.2</th>\n",
       "      <th>key_resp.rt.3</th>\n",
       "      <th>key_resp.rt.4</th>\n",
       "      <th>key_resp.rt.5</th>\n",
       "      <th>key_resp.rt.6</th>\n",
       "      <th>wf_type</th>\n",
       "      <th>meanbf_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>faqir</td>\n",
       "      <td>faqir</td>\n",
       "      <td>fa</td>\n",
       "      <td>aq</td>\n",
       "      <td>qi</td>\n",
       "      <td>ir</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>r</td>\n",
       "      <td>None</td>\n",
       "      <td>1.172795</td>\n",
       "      <td>1.324863</td>\n",
       "      <td>2.060788</td>\n",
       "      <td>2.612769</td>\n",
       "      <td>2.708730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>lowwf</td>\n",
       "      <td>lowbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s302</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>heond</td>\n",
       "      <td>heond</td>\n",
       "      <td>he</td>\n",
       "      <td>eo</td>\n",
       "      <td>on</td>\n",
       "      <td>nd</td>\n",
       "      <td>h</td>\n",
       "      <td>...</td>\n",
       "      <td>d</td>\n",
       "      <td>None</td>\n",
       "      <td>0.857461</td>\n",
       "      <td>0.985568</td>\n",
       "      <td>1.137400</td>\n",
       "      <td>1.321348</td>\n",
       "      <td>1.457272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>pseudo</td>\n",
       "      <td>highbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s302</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>pykka</td>\n",
       "      <td>pykka</td>\n",
       "      <td>py</td>\n",
       "      <td>yk</td>\n",
       "      <td>kk</td>\n",
       "      <td>ka</td>\n",
       "      <td>p</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "      <td>0.973379</td>\n",
       "      <td>1.245340</td>\n",
       "      <td>1.557428</td>\n",
       "      <td>1.677379</td>\n",
       "      <td>1.773341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>pseudo</td>\n",
       "      <td>lowbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s302</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>belly</td>\n",
       "      <td>belly</td>\n",
       "      <td>be</td>\n",
       "      <td>el</td>\n",
       "      <td>ll</td>\n",
       "      <td>ly</td>\n",
       "      <td>b</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>None</td>\n",
       "      <td>0.905309</td>\n",
       "      <td>1.121302</td>\n",
       "      <td>1.297236</td>\n",
       "      <td>1.409406</td>\n",
       "      <td>1.617255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>medwf</td>\n",
       "      <td>medbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s302</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>champ</td>\n",
       "      <td>chamnp</td>\n",
       "      <td>ch</td>\n",
       "      <td>ha</td>\n",
       "      <td>am</td>\n",
       "      <td>mp</td>\n",
       "      <td>c</td>\n",
       "      <td>...</td>\n",
       "      <td>n</td>\n",
       "      <td>p</td>\n",
       "      <td>0.733351</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.933277</td>\n",
       "      <td>1.109470</td>\n",
       "      <td>1.109470</td>\n",
       "      <td>1.389263</td>\n",
       "      <td>medwf</td>\n",
       "      <td>medbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>s302</td>\n",
       "      <td>235</td>\n",
       "      <td>9</td>\n",
       "      <td>puppy</td>\n",
       "      <td>puppy</td>\n",
       "      <td>pu</td>\n",
       "      <td>up</td>\n",
       "      <td>pp</td>\n",
       "      <td>py</td>\n",
       "      <td>p</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>None</td>\n",
       "      <td>0.568009</td>\n",
       "      <td>0.728052</td>\n",
       "      <td>0.952085</td>\n",
       "      <td>1.072238</td>\n",
       "      <td>1.232043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>medwf</td>\n",
       "      <td>lowbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>s302</td>\n",
       "      <td>236</td>\n",
       "      <td>9</td>\n",
       "      <td>would</td>\n",
       "      <td>would</td>\n",
       "      <td>wo</td>\n",
       "      <td>ou</td>\n",
       "      <td>ul</td>\n",
       "      <td>ld</td>\n",
       "      <td>w</td>\n",
       "      <td>...</td>\n",
       "      <td>d</td>\n",
       "      <td>None</td>\n",
       "      <td>0.691983</td>\n",
       "      <td>0.787944</td>\n",
       "      <td>0.867945</td>\n",
       "      <td>1.027949</td>\n",
       "      <td>1.148070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>highwf</td>\n",
       "      <td>medbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>s302</td>\n",
       "      <td>237</td>\n",
       "      <td>9</td>\n",
       "      <td>champ</td>\n",
       "      <td>champ</td>\n",
       "      <td>ch</td>\n",
       "      <td>ha</td>\n",
       "      <td>am</td>\n",
       "      <td>mp</td>\n",
       "      <td>c</td>\n",
       "      <td>...</td>\n",
       "      <td>p</td>\n",
       "      <td>None</td>\n",
       "      <td>0.608112</td>\n",
       "      <td>0.704019</td>\n",
       "      <td>0.831890</td>\n",
       "      <td>0.951992</td>\n",
       "      <td>1.151988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>medwf</td>\n",
       "      <td>medbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>s302</td>\n",
       "      <td>238</td>\n",
       "      <td>9</td>\n",
       "      <td>edthe</td>\n",
       "      <td>edthe</td>\n",
       "      <td>ed</td>\n",
       "      <td>dt</td>\n",
       "      <td>th</td>\n",
       "      <td>he</td>\n",
       "      <td>e</td>\n",
       "      <td>...</td>\n",
       "      <td>e</td>\n",
       "      <td>None</td>\n",
       "      <td>0.564038</td>\n",
       "      <td>0.771991</td>\n",
       "      <td>0.947868</td>\n",
       "      <td>1.051883</td>\n",
       "      <td>1.107851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>pseudo</td>\n",
       "      <td>highbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>s302</td>\n",
       "      <td>239</td>\n",
       "      <td>9</td>\n",
       "      <td>zibja</td>\n",
       "      <td>zibja</td>\n",
       "      <td>zi</td>\n",
       "      <td>ib</td>\n",
       "      <td>bj</td>\n",
       "      <td>ja</td>\n",
       "      <td>z</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>None</td>\n",
       "      <td>1.079913</td>\n",
       "      <td>1.215810</td>\n",
       "      <td>1.575824</td>\n",
       "      <td>1.823839</td>\n",
       "      <td>1.959738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>pseudo</td>\n",
       "      <td>lowbf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sID  trial_num  rep_num string resp_string bi_1 bi_2 bi_3 bi_4  \\\n",
       "0    s302          0        0  faqir       faqir   fa   aq   qi   ir   \n",
       "1    s302          1        0  heond       heond   he   eo   on   nd   \n",
       "2    s302          2        0  pykka       pykka   py   yk   kk   ka   \n",
       "3    s302          3        0  belly       belly   be   el   ll   ly   \n",
       "4    s302          4        0  champ      chamnp   ch   ha   am   mp   \n",
       "..    ...        ...      ...    ...         ...  ...  ...  ...  ...   \n",
       "235  s302        235        9  puppy       puppy   pu   up   pp   py   \n",
       "236  s302        236        9  would       would   wo   ou   ul   ld   \n",
       "237  s302        237        9  champ       champ   ch   ha   am   mp   \n",
       "238  s302        238        9  edthe       edthe   ed   dt   th   he   \n",
       "239  s302        239        9  zibja       zibja   zi   ib   bj   ja   \n",
       "\n",
       "    key_resp.keys.1  ... key_resp.keys.5 key_resp.keys.6 key_resp.rt.1  \\\n",
       "0                 f  ...               r            None      1.172795   \n",
       "1                 h  ...               d            None      0.857461   \n",
       "2                 p  ...               a            None      0.973379   \n",
       "3                 b  ...               y            None      0.905309   \n",
       "4                 c  ...               n               p      0.733351   \n",
       "..              ...  ...             ...             ...           ...   \n",
       "235               p  ...               y            None      0.568009   \n",
       "236               w  ...               d            None      0.691983   \n",
       "237               c  ...               p            None      0.608112   \n",
       "238               e  ...               e            None      0.564038   \n",
       "239               z  ...               a            None      1.079913   \n",
       "\n",
       "    key_resp.rt.2 key_resp.rt.3  key_resp.rt.4  key_resp.rt.5  key_resp.rt.6  \\\n",
       "0        1.324863      2.060788       2.612769       2.708730       0.000000   \n",
       "1        0.985568      1.137400       1.321348       1.457272       0.000000   \n",
       "2        1.245340      1.557428       1.677379       1.773341       0.000000   \n",
       "3        1.121302      1.297236       1.409406       1.617255       0.000000   \n",
       "4        0.837500      0.933277       1.109470       1.109470       1.389263   \n",
       "..            ...           ...            ...            ...            ...   \n",
       "235      0.728052      0.952085       1.072238       1.232043       0.000000   \n",
       "236      0.787944      0.867945       1.027949       1.148070       0.000000   \n",
       "237      0.704019      0.831890       0.951992       1.151988       0.000000   \n",
       "238      0.771991      0.947868       1.051883       1.107851       0.000000   \n",
       "239      1.215810      1.575824       1.823839       1.959738       0.000000   \n",
       "\n",
       "     wf_type  meanbf_type  \n",
       "0      lowwf        lowbf  \n",
       "1     pseudo       highbf  \n",
       "2     pseudo        lowbf  \n",
       "3      medwf        medbf  \n",
       "4      medwf        medbf  \n",
       "..       ...          ...  \n",
       "235    medwf        lowbf  \n",
       "236   highwf        medbf  \n",
       "237    medwf        medbf  \n",
       "238   pseudo       highbf  \n",
       "239   pseudo        lowbf  \n",
       "\n",
       "[240 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can also be specified\n",
    "#     print(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sID</th>\n",
       "      <th>bigram_num</th>\n",
       "      <th>trial_num</th>\n",
       "      <th>rep_num</th>\n",
       "      <th>bigram_loc</th>\n",
       "      <th>bigram</th>\n",
       "      <th>resp_bigram</th>\n",
       "      <th>IKI</th>\n",
       "      <th>string</th>\n",
       "      <th>resp_string</th>\n",
       "      <th>bg_freq</th>\n",
       "      <th>bf_type</th>\n",
       "      <th>meanbf_type</th>\n",
       "      <th>wf_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fa</td>\n",
       "      <td>fa</td>\n",
       "      <td>0.152068</td>\n",
       "      <td>faqir</td>\n",
       "      <td>faqir</td>\n",
       "      <td>1043958.0</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s302</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>aq</td>\n",
       "      <td>aq</td>\n",
       "      <td>0.735925</td>\n",
       "      <td>faqir</td>\n",
       "      <td>faqir</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s302</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>qi</td>\n",
       "      <td>qi</td>\n",
       "      <td>0.551981</td>\n",
       "      <td>faqir</td>\n",
       "      <td>faqir</td>\n",
       "      <td>36.0</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s302</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>ir</td>\n",
       "      <td>ir</td>\n",
       "      <td>0.095961</td>\n",
       "      <td>faqir</td>\n",
       "      <td>faqir</td>\n",
       "      <td>2142678.0</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s302</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>he</td>\n",
       "      <td>he</td>\n",
       "      <td>0.128107</td>\n",
       "      <td>heond</td>\n",
       "      <td>heond</td>\n",
       "      <td>21484684.0</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>pseudo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>s302</td>\n",
       "      <td>950</td>\n",
       "      <td>238</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>he</td>\n",
       "      <td>he</td>\n",
       "      <td>0.055968</td>\n",
       "      <td>edthe</td>\n",
       "      <td>edthe</td>\n",
       "      <td>21484684.0</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>pseudo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>s302</td>\n",
       "      <td>951</td>\n",
       "      <td>239</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>zi</td>\n",
       "      <td>zi</td>\n",
       "      <td>0.135897</td>\n",
       "      <td>zibja</td>\n",
       "      <td>zibja</td>\n",
       "      <td>54565.0</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>pseudo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>s302</td>\n",
       "      <td>952</td>\n",
       "      <td>239</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>ib</td>\n",
       "      <td>ib</td>\n",
       "      <td>0.360014</td>\n",
       "      <td>zibja</td>\n",
       "      <td>zibja</td>\n",
       "      <td>493267.0</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>pseudo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>s302</td>\n",
       "      <td>953</td>\n",
       "      <td>239</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>bj</td>\n",
       "      <td>bj</td>\n",
       "      <td>0.248015</td>\n",
       "      <td>zibja</td>\n",
       "      <td>zibja</td>\n",
       "      <td>105433.0</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>pseudo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>s302</td>\n",
       "      <td>954</td>\n",
       "      <td>239</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>ja</td>\n",
       "      <td>ja</td>\n",
       "      <td>0.135898</td>\n",
       "      <td>zibja</td>\n",
       "      <td>zibja</td>\n",
       "      <td>40073.0</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>pseudo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>955 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sID  bigram_num  trial_num  rep_num  bigram_loc bigram resp_bigram  \\\n",
       "0    s302           0          0        0           0     fa          fa   \n",
       "1    s302           1          0        0           1     aq          aq   \n",
       "2    s302           2          0        0           2     qi          qi   \n",
       "3    s302           3          0        0           3     ir          ir   \n",
       "4    s302           4          1        0           0     he          he   \n",
       "..    ...         ...        ...      ...         ...    ...         ...   \n",
       "950  s302         950        238        8           3     he          he   \n",
       "951  s302         951        239        9           0     zi          zi   \n",
       "952  s302         952        239        9           1     ib          ib   \n",
       "953  s302         953        239        9           2     bj          bj   \n",
       "954  s302         954        239        9           3     ja          ja   \n",
       "\n",
       "          IKI string resp_string     bg_freq bf_type meanbf_type wf_type  \n",
       "0    0.152068  faqir       faqir   1043958.0     med         low     low  \n",
       "1    0.735925  faqir       faqir      4924.0     low         low     low  \n",
       "2    0.551981  faqir       faqir        36.0     low         low     low  \n",
       "3    0.095961  faqir       faqir   2142678.0     med         low     low  \n",
       "4    0.128107  heond       heond  21484684.0    high        high  pseudo  \n",
       "..        ...    ...         ...         ...     ...         ...     ...  \n",
       "950  0.055968  edthe       edthe  21484684.0    high        high  pseudo  \n",
       "951  0.135897  zibja       zibja     54565.0     low         low  pseudo  \n",
       "952  0.360014  zibja       zibja    493267.0     low         low  pseudo  \n",
       "953  0.248015  zibja       zibja    105433.0     low         low  pseudo  \n",
       "954  0.135898  zibja       zibja     40073.0     low         low  pseudo  \n",
       "\n",
       "[955 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_df\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can also be specified\n",
    "#     print(bigram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
